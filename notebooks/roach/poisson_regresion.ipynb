{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josh/workspace/bayesianquilts/env/lib/python3.11/site-packages/arviz/__init__.py:39: FutureWarning: \n",
      "ArviZ is undergoing a major refactor to improve flexibility and extensibility while maintaining a user-friendly interface.\n",
      "Some upcoming changes may be backward incompatible.\n",
      "For details and migration guidance, visit: https://python.arviz.org/en/latest/user_guide/migration_guide.html\n",
      "  warn(\n",
      "/var/folders/9g/95lvk8690_52tvn55sr5m9nh0000gn/T/ipykernel_54581/2465634065.py:12: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "import os\n",
    "import pkg_resources\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "from bayesianquilts.model import BayesianModel\n",
    "from bayesianquilts.vi.minibatch import minibatch_fit_surrogate_posterior\n",
    "from bayesianquilts.metrics.ais import AdaptiveImportanceSampler, PoissonRegressionLikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/josh/workspace/bayesianquilts/bayesianquilts/data/roachdata.csv\n",
      "   Unnamed: 0    y  roach1  treatment  senior  exposure2\n",
      "0           1  153  308.00          1       0   0.800000\n",
      "1           2  127  331.25          1       0   0.600000\n",
      "2           3    7    1.67          1       0   1.000000\n",
      "3           4    7    3.00          1       0   1.000000\n",
      "4           5    0    2.00          1       0   1.142857\n",
      "X shape: (262, 4), y shape: (262,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    data_path = pkg_resources.resource_filename('bayesianquilts.data', 'roachdata.csv')\n",
    "except ImportError:\n",
    "    # Fallback if package not installed\n",
    "    data_path = '../../bayesianquilts/data/roachdata.csv'\n",
    "    if not os.path.exists(data_path):\n",
    "        data_path = 'bayesianquilts/data/roachdata.csv'\n",
    "\n",
    "print(f'Loading data from {data_path}')\n",
    "df = pd.read_csv(data_path)\n",
    "print(df.head())\n",
    "\n",
    "# Preprocessing\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Target\n",
    "y_data = df['y'].values\n",
    "\n",
    "# Features\n",
    "# Drop target and any index columns\n",
    "X_df = df.drop(columns=['y'])\n",
    "X_data = X_df.values.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "print(f'X shape: {X_data.shape}, y shape: {y_data.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoissonRegression(BayesianModel):\n",
    "    def __init__(self, input_dim, dtype=jnp.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.var_list = ['beta', 'intercept']\n",
    "        self.create_distributions()\n",
    "\n",
    "    def create_distributions(self):\n",
    "        # Surrogate: Mean-field Normal\n",
    "        self.surrogate_distribution = tfd.JointDistributionNamedAutoBatched({\n",
    "            'beta': tfd.Normal(\n",
    "                loc=jax.nn.initializers.zeros(jax.random.PRNGKey(0), (self.input_dim,), dtype=self.dtype),\n",
    "                scale=1e-2 + jax.nn.softplus(jax.nn.initializers.uniform(scale=0.01)(jax.random.PRNGKey(1), (self.input_dim,), dtype=self.dtype))\n",
    "            ),\n",
    "            'intercept': tfd.Normal(\n",
    "                loc=jax.nn.initializers.zeros(jax.random.PRNGKey(2), (1,), dtype=self.dtype),\n",
    "                scale=1e-2 + jax.nn.softplus(jax.nn.initializers.uniform(scale=0.01)(jax.random.PRNGKey(3), (1,), dtype=self.dtype))\n",
    "            )\n",
    "        })\n",
    "\n",
    "    def surrogate_parameter_initializer(self, key=None, **kwargs):\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(42)\n",
    "        k1, k2 = jax.random.split(key, 2)\n",
    "        \n",
    "        def init_mean_scale(shape, k):\n",
    "            ska, skb = jax.random.split(k)\n",
    "            mean = jax.random.normal(ska, shape, dtype=self.dtype) * 0.01\n",
    "            raw_scale = jnp.log(jnp.exp(0.01) - 1.0) + jax.random.normal(skb, shape, dtype=self.dtype) * 0.001\n",
    "            return mean, raw_scale\n",
    "\n",
    "        beta_loc, beta_scale = init_mean_scale((self.input_dim,), k1)\n",
    "        int_loc, int_scale = init_mean_scale((1,), k2)\n",
    "\n",
    "        return {\n",
    "            'beta_loc': beta_loc, 'beta_raw_scale': beta_scale,\n",
    "            'intercept_loc': int_loc, 'intercept_raw_scale': int_scale\n",
    "        }\n",
    "\n",
    "    def surrogate_distribution_generator(self, params):\n",
    "        return tfd.JointDistributionNamed({\n",
    "            'beta': tfd.Independent(tfd.Normal(params['beta_loc'], jax.nn.softplus(params['beta_raw_scale']) + 1e-5), reinterpreted_batch_ndims=1),\n",
    "            'intercept': tfd.Independent(tfd.Normal(params['intercept_loc'], jax.nn.softplus(params['intercept_raw_scale']) + 1e-5), reinterpreted_batch_ndims=1)\n",
    "        })\n",
    "\n",
    "    def unormalized_log_prob(self, data, prior_weight=1.0, **params):\n",
    "        X = data['X']\n",
    "        y = data['y']\n",
    "        \n",
    "        beta = params['beta']\n",
    "        intercept = params['intercept']\n",
    "        \n",
    "        has_sample_dim = (beta.ndim > 1)\n",
    "        \n",
    "        # Priors\n",
    "        if has_sample_dim:\n",
    "             p_beta = jnp.sum(tfd.Normal(0., 10.).log_prob(beta), axis=-1)\n",
    "             p_int = jnp.sum(tfd.Normal(0., 10.).log_prob(intercept), axis=-1)\n",
    "             log_prior = p_beta + p_int\n",
    "             \n",
    "             # Likelihood\n",
    "             # X: (B, D), beta: (S, D) -> (S, B)\n",
    "             eta = jnp.einsum('bd,sd->sb', X, beta) + intercept\n",
    "        else:\n",
    "            log_prior = jnp.sum(tfd.Normal(0., 10.).log_prob(beta)) + jnp.sum(tfd.Normal(0., 10.).log_prob(intercept))\n",
    "            eta = jnp.dot(X, beta) + intercept\n",
    "            \n",
    "        rate = jnp.exp(eta)\n",
    "        \n",
    "        # Poisson Log Likelihood\n",
    "        if has_sample_dim:\n",
    "            log_lik = tfd.Poisson(rate=rate).log_prob(y)\n",
    "            log_lik = jnp.sum(log_lik, axis=-1)\n",
    "        else:\n",
    "            log_lik = jnp.sum(tfd.Poisson(rate=rate).log_prob(y))\n",
    "            \n",
    "        return log_lik + log_prior * prior_weight\n",
    "\n",
    "    def log_likelihood(self, data, **params):\n",
    "        X = data['X']\n",
    "        y = data['y']\n",
    "        beta = params['beta']\n",
    "        intercept = params['intercept']\n",
    "        \n",
    "        has_sample_dim = (beta.ndim > 1)\n",
    "        \n",
    "        if has_sample_dim:\n",
    "            eta = jnp.einsum('bd,sd->sb', X, beta) + intercept\n",
    "        else:\n",
    "            eta = jnp.dot(X, beta) + intercept\n",
    "            \n",
    "        rate = jnp.exp(eta)\n",
    "        # Returns (S, B) or (B,)\n",
    "        return tfd.Poisson(rate=rate).log_prob(y)\n",
    "\n",
    "    def predictive_distribution(self, data, **params):\n",
    "        X = data['X']\n",
    "        beta = params['beta']\n",
    "        intercept = params['intercept']\n",
    "        has_sample_dim = (beta.ndim > 1)\n",
    "        if has_sample_dim:\n",
    "            eta = jnp.einsum('bd,sd->sb', X, beta) + intercept\n",
    "        else:\n",
    "            eta = jnp.dot(X, beta) + intercept\n",
    "        rate = jnp.exp(eta)\n",
    "        \n",
    "        log_lik = None\n",
    "        if 'y' in data:\n",
    "            log_lik = tfd.Poisson(rate=rate).log_prob(data['y'])\n",
    "            \n",
    "        return {'prediction': rate, 'log_likelihood': log_lik}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_JointDistributionNamedAutoBatchedSpec' object has no attribute '_structure_with_callables'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mPoissonRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Data factory for batching\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdata_factory_builder\u001b[39m(batch_size=\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/flax/nnx/pytreelib.py:400\u001b[39m, in \u001b[36mPytreeMeta.__call__\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_graph_node_meta_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/flax/nnx/pytreelib.py:412\u001b[39m, in \u001b[36m_graph_node_meta_call\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m vars_obj[\u001b[33m'\u001b[39m\u001b[33m_pytree__state\u001b[39m\u001b[33m'\u001b[39m] = PytreeState()\n\u001b[32m    411\u001b[39m vars_obj[\u001b[33m'\u001b[39m\u001b[33m_pytree__nodes\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mcls\u001b[39m._pytree__nodes\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pytree_meta_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._pytree__is_pytree:\n\u001b[32m    414\u001b[39m   missing: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m] = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/flax/nnx/pytreelib.py:403\u001b[39m, in \u001b[36mPytreeMeta._pytree_meta_construct\u001b[39m\u001b[34m(cls, self, *args, **kwargs)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pytree_meta_construct\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mPoissonRegression.__init__\u001b[39m\u001b[34m(self, input_dim, dtype, **kwargs)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mself\u001b[39m.input_dim = input_dim\n\u001b[32m      5\u001b[39m \u001b[38;5;28mself\u001b[39m.var_list = [\u001b[33m'\u001b[39m\u001b[33mbeta\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mintercept\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_distributions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mPoissonRegression.create_distributions\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_distributions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Surrogate: Mean-field Normal\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msurrogate_distribution\u001b[49m = tfd.JointDistributionNamedAutoBatched({\n\u001b[32m     11\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mbeta\u001b[39m\u001b[33m'\u001b[39m: tfd.Normal(\n\u001b[32m     12\u001b[39m             loc=jax.nn.initializers.zeros(jax.random.PRNGKey(\u001b[32m0\u001b[39m), (\u001b[38;5;28mself\u001b[39m.input_dim,), dtype=\u001b[38;5;28mself\u001b[39m.dtype),\n\u001b[32m     13\u001b[39m             scale=\u001b[32m1e-2\u001b[39m + jax.nn.softplus(jax.nn.initializers.uniform(scale=\u001b[32m0.01\u001b[39m)(jax.random.PRNGKey(\u001b[32m1\u001b[39m), (\u001b[38;5;28mself\u001b[39m.input_dim,), dtype=\u001b[38;5;28mself\u001b[39m.dtype))\n\u001b[32m     14\u001b[39m         ),\n\u001b[32m     15\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mintercept\u001b[39m\u001b[33m'\u001b[39m: tfd.Normal(\n\u001b[32m     16\u001b[39m             loc=jax.nn.initializers.zeros(jax.random.PRNGKey(\u001b[32m2\u001b[39m), (\u001b[32m1\u001b[39m,), dtype=\u001b[38;5;28mself\u001b[39m.dtype),\n\u001b[32m     17\u001b[39m             scale=\u001b[32m1e-2\u001b[39m + jax.nn.softplus(jax.nn.initializers.uniform(scale=\u001b[32m0.01\u001b[39m)(jax.random.PRNGKey(\u001b[32m3\u001b[39m), (\u001b[32m1\u001b[39m,), dtype=\u001b[38;5;28mself\u001b[39m.dtype))\n\u001b[32m     18\u001b[39m         )\n\u001b[32m     19\u001b[39m     })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/flax/nnx/pytreelib.py:619\u001b[39m, in \u001b[36mPytree.__setattr__\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, value: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setattr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/flax/nnx/pytreelib.py:636\u001b[39m, in \u001b[36mPytree._setattr\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m    634\u001b[39m   data = is_data(value)\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pytree__is_pytree:\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAttributeStatus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplicit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pytree__nodes \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    638\u001b[39m       explicit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pytree__nodes[name] != data\n\u001b[32m    639\u001b[39m   ):\n\u001b[32m    640\u001b[39m     \u001b[38;5;28mvars\u001b[39m(\u001b[38;5;28mself\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33m_pytree__nodes\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m._pytree__nodes.update({name: data})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/flax/nnx/pytreelib.py:664\u001b[39m, in \u001b[36mPytree._check_value\u001b[39m\u001b[34m(self, key, value, new_status)\u001b[39m\n\u001b[32m    661\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    663\u001b[39m visited: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m664\u001b[39m leaves = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mleaves\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_has_visited\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m current_is_data = (\n\u001b[32m    666\u001b[39m     \u001b[38;5;28mself\u001b[39m._pytree__nodes[key] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pytree__nodes \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m )\n\u001b[32m    668\u001b[39m existing_attr = key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/tree.py:112\u001b[39m, in \u001b[36mleaves\u001b[39m\u001b[34m(tree, is_leaf)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mleaves\u001b[39m(tree: Any,\n\u001b[32m     88\u001b[39m            is_leaf: Callable[[Any], \u001b[38;5;28mbool\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     89\u001b[39m            ) -> \u001b[38;5;28mlist\u001b[39m[tree_util.Leaf]:\n\u001b[32m     90\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Gets the leaves of a pytree.\u001b[39;00m\n\u001b[32m     91\u001b[39m \n\u001b[32m     92\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m \u001b[33;03m    - :func:`jax.tree.unflatten`\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_util\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree_leaves\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/tree_util.py:93\u001b[39m, in \u001b[36mtree_leaves\u001b[39m\u001b[34m(tree, is_leaf)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;129m@export\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtree_leaves\u001b[39m(tree: Any,\n\u001b[32m     90\u001b[39m                 is_leaf: Callable[[Any], \u001b[38;5;28mbool\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     91\u001b[39m                 ) -> \u001b[38;5;28mlist\u001b[39m[Leaf]:\n\u001b[32m     92\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Alias of :func:`jax.tree.leaves`.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/distributions/joint_distribution_named.py:575\u001b[39m, in \u001b[36m_pytree_flatten\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    572\u001b[39m   keys, values = (), ()\n\u001b[32m    573\u001b[39m metadata = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    574\u001b[39m     non_tensor_params=obj._type_spec._non_tensor_params,\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     structure_with_callables=\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_type_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_structure_with_callables\u001b[49m)\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values, (keys, metadata)\n",
      "\u001b[31mAttributeError\u001b[39m: '_JointDistributionNamedAutoBatchedSpec' object has no attribute '_structure_with_callables'"
     ]
    }
   ],
   "source": [
    "model = PoissonRegression(input_dim=X_data.shape[1])\n",
    "\n",
    "# Data factory for batching\n",
    "def data_factory_builder(batch_size=None):\n",
    "    if batch_size is None:\n",
    "        batch_size = len(X_data)\n",
    "    \n",
    "    num_batches = int(np.ceil(len(X_data) / batch_size))\n",
    "    \n",
    "    def generator():\n",
    "        indices = np.arange(len(X_data))\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(num_batches):\n",
    "            idx = indices[i*batch_size : (i+1)*batch_size]\n",
    "            if len(idx) > 0:\n",
    "                yield {'X': X_data[idx], 'y': y_data[idx]}\n",
    "            \n",
    "    return generator\n",
    "\n",
    "# Check init\n",
    "print('Checking initialization...')\n",
    "try:\n",
    "    params = model.surrogate_distribution.sample(2)\n",
    "    print('Log prob check:', model.unormalized_log_prob({'X': X_data, 'y': y_data}, **params))\n",
    "except Exception as e:\n",
    "    print(f'Initialization failed: {e}')\n",
    "    pass\n",
    "\n",
    "# Fit\n",
    "print('Fitting model...')\n",
    "batch_size = 32\n",
    "losses, params = model.fit(\n",
    "    batched_data_factory=data_factory_builder(batch_size),\n",
    "    batch_size=batch_size,\n",
    "    dataset_size=len(X_data),\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.01,\n",
    "    patience=20\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('ADVI Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation with AIS\n",
    "print('Computing LOO-IC using Adaptive Importance Sampling (AIS)...')\n",
    "\n",
    "# Initialize Likelihood\n",
    "likelihood_fn = PoissonRegressionLikelihood()\n",
    "\n",
    "# Create Sampler\n",
    "ais_sampler = AdaptiveImportanceSampler(likelihood_fn=likelihood_fn)\n",
    "\n",
    "# Prepare data and params\n",
    "# We need to extract params from the model and format them as expected by AIS\n",
    "# AIS expects params as a dictionary of arrays with shape (n_samples, ...)\n",
    "params = model.surrogate_distribution.sample(100) # Sample from posterior/surrogate\n",
    "\n",
    "# Run AIS LOO\n",
    "# We need to pass data as a dictionary {'X': ..., 'y': ...}\n",
    "# Ensure data is jax arrays\n",
    "data_jax = {'X': jnp.array(X_data), 'y': jnp.array(y_data)}\n",
    "\n",
    "results = ais_sampler.adaptive_is_loo(\n",
    "    data=data_jax,\n",
    "    params=params,\n",
    "    hbar=1.0,\n",
    "    variational=True, # We used VI\n",
    "    transformations=['ll', 'kl', 'var', 'identity']\n",
    ")\n",
    "\n",
    "for method, res in results.items():\n",
    "    print(f'Method: {method}')\n",
    "    print(f'  LOO-IC (eta): {-2 * jnp.sum(jnp.log(res[\"p_loo_eta\"]))}')\n",
    "    print(f'  LOO-IC (psis): {-2 * jnp.sum(jnp.log(res[\"p_loo_psis\"]))}')\n",
    "    print(f'  Pareto k (min/max): {res[\"khat\"].min():.3f}/{res[\"khat\"].max():.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
