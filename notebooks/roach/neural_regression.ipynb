{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5544b7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9g/95lvk8690_52tvn55sr5m9nh0000gn/T/ipykernel_23021/2474793834.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched _JointDistributionNamedSpec with _structure_with_callables=None\n",
      "TFP Specs patched successfully.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "import os\n",
    "import pickle\n",
    "import importlib.resources\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "from bayesianquilts.model import BayesianModel\n",
    "from bayesianquilts.vi.minibatch import minibatch_fit_surrogate_posterior\n",
    "from bayesianquilts.metrics.ais import AdaptiveImportanceSampler\n",
    "from bayesianquilts.predictors.nn.negbin import NeuralNegativeBinomialRegression\n",
    "from bayesianquilts.predictors.nn.negbin import NeuralNegativeBinomialLikelihood\n",
    "from bayesianquilts.predictors.nn.poisson import NeuralPoissonLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d375b74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/josh/workspace/bayesianquilts/bayesianquilts/data/roachdata.csv\n",
      "   Unnamed: 0    y  roach1  treatment  senior  exposure2\n",
      "0           1  153  308.00          1       0   0.800000\n",
      "1           2  127  331.25          1       0   0.600000\n",
      "2           3    7    1.67          1       0   1.000000\n",
      "3           4    7    3.00          1       0   1.000000\n",
      "4           5    0    2.00          1       0   1.142857\n",
      "Features normalized (mean=0, std=1)\n",
      "X shape: (262, 4), y shape: (262,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    data_path = str(importlib.resources.files('bayesianquilts.data').joinpath('roachdata.csv'))\n",
    "except ImportError:\n",
    "    # Fallback if package not installed\n",
    "    data_path = '../../bayesianquilts/data/roachdata.csv'\n",
    "    if not os.path.exists(data_path):\n",
    "        data_path = 'bayesianquilts/data/roachdata.csv'\n",
    "\n",
    "print(f'Loading data from {data_path}')\n",
    "df = pd.read_csv(data_path)\n",
    "print(df.head())\n",
    "\n",
    "# Preprocessing\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Target\n",
    "y_data = df['y'].values\n",
    "\n",
    "# Features\n",
    "# Drop target and any index columns\n",
    "X_df = df.drop(columns=['y'])\n",
    "X_data = X_df.values.astype(np.float32)\n",
    "\n",
    "# Normalize features\n",
    "mean = X_data.mean(axis=0)\n",
    "std = X_data.std(axis=0)\n",
    "X_data = (X_data - mean) / std\n",
    "print('Features normalized (mean=0, std=1)')\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "print(f'X shape: {X_data.shape}, y shape: {y_data.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12b5b10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable (y) statistics:\n",
      "  Min: 0.00\n",
      "  Max: 357.00\n",
      "  Mean: 25.65\n",
      "  Std: 50.75\n",
      "  Zeros: 94 / 262 (35.9%)\n",
      "  y[y>0] mean: 40.00\n",
      "\n",
      "Model: NeuralNegativeBinomialRegression (Zero-Inflated)\n",
      "  - Likelihood: Zero-Inflated Negative Binomial\n",
      "  - Output scale: 25.65 (mean of y)\n",
      "  - Network outputs: 3 (zero_logit, log_mean, log_concentration)\n",
      "  - Hidden layers: 2 \u00d7 4 neurons\n",
      "  - Activation: ReLU\n",
      "  - Handles overdispersion and excess zeros\n",
      "\n",
      "Checking initialization...\n",
      "Initial sample keys: dict_keys(['w_2', 'w_1', 'w_0', 'b_2', 'b_1', 'b_0'])\n",
      "\n",
      "======================================================================\n",
      "PATHFINDER INITIALIZATION STRATEGY (Conservative Settings)\n",
      "======================================================================\n",
      "\n",
      "No cached model found. Starting fresh fit with Pathfinder...\n",
      "\n",
      "Step 1: Running Pathfinder variational inference...\n",
      "  L-BFGS settings: ftol=1e-06, gtol=1e-09 (conservative)\n",
      "  Parameter space dimension: 55\n",
      "  \u2713 Pathfinder converged! ELBO: -1124.760\n",
      "\n",
      "Step 2: Sampling 4 diverse initial states...\n",
      "  \u2713 Sampled from approximate posterior\n",
      "  w_0: shape (4, 4, 4)\n",
      "  b_0: shape (4, 4)\n",
      "  w_1: shape (4, 4, 4)\n",
      "  b_1: shape (4, 4)\n",
      "  w_2: shape (4, 4, 3)\n",
      "  b_2: shape (4, 3)\n",
      "  \u2713 Initial states ready for MCMC\n",
      "\n",
      "Step 3: Running MCMC with Pathfinder-initialized chains...\n",
      "Running NUTS with 4 chains...\n",
      "  Warmup: 10000, Samples: 2000\n",
      "  Target acceptance: 0.85, Max tree depth: 10\n",
      "  Using custom initial states\n",
      "\n",
      "Chain 1/4:\n",
      "  Running warmup and sampling...\n",
      "  Acceptance ratio: 1.000\n",
      "\n",
      "Chain 2/4:\n",
      "  Running warmup and sampling...\n",
      "  Acceptance ratio: 1.000\n",
      "\n",
      "Chain 3/4:\n",
      "  Running warmup and sampling...\n",
      "  Acceptance ratio: 1.000\n",
      "\n",
      "Chain 4/4:\n",
      "  Running warmup and sampling...\n",
      "  Acceptance ratio: 1.000\n",
      "\n",
      "--- MCMC Complete ---\n",
      "Mean acceptance ratio: 1.000\n",
      "  w_0: mean=[[-0.1658253  -0.2174373  -0.45481005 -0.03593464]\n",
      " [-0.04412386  0.00973953 -0.04196937 -0.25421844]\n",
      " [ 0.14742411  0.37038917  0.80173271 -0.02656123]\n",
      " [ 0.41879958  0.20850356  0.4213473   0.32881991]], std=[[0.38480068 0.79493416 0.49627771 0.23106632]\n",
      " [0.25783887 0.55475814 0.33855058 0.64214086]\n",
      " [0.22095058 0.5331227  0.67546029 0.21835628]\n",
      " [0.31308678 0.64461707 1.00332124 0.39330558]], max_rhat=149.107\n",
      "  b_0: mean=[-0.14173884 -0.30555604  0.11914818 -0.18383254], std=[0.36399942 0.61333634 0.55199987 0.89246512], max_rhat=121.110\n",
      "  w_1: mean=[[-0.2361854   2.10845194 -0.18630687 -0.24013238]\n",
      " [ 0.00951771  0.02604347  0.42003585 -0.00363255]\n",
      " [ 0.09430649 -0.52888603 -0.31650701 -0.15698328]\n",
      " [-0.05901768  0.14387955  0.76606742  0.16938509]], std=[[0.45607613 0.82425069 0.26320182 0.44926511]\n",
      " [0.1541886  0.40189281 0.42474767 0.20465946]\n",
      " [0.84895252 0.30821937 0.48697659 0.24353703]\n",
      " [0.2116576  0.44300916 0.69008946 0.53037913]], max_rhat=45.666\n",
      "  b_1: mean=[ 0.05091916 -0.16596173 -0.00736407  0.57064638], std=[0.39709136 0.58000287 0.37670157 0.80824104], max_rhat=49.217\n",
      "  w_2: mean=[[ 0.20175673 -0.27223357  0.10461819]\n",
      " [ 0.03713921 -0.63834294  0.05484604]\n",
      " [-0.33991369 -0.24645384 -0.02488541]\n",
      " [-0.22167253  0.12789457 -0.02916194]], std=[[0.93571296 0.79540756 0.50662588]\n",
      " [0.13098896 0.44435208 0.13673417]\n",
      " [0.8111537  0.22206077 0.50950254]\n",
      " [1.07768409 0.1798734  0.22712285]], max_rhat=185.080\n",
      "  b_2: mean=[-1.50026195 -8.92501314 -1.10995927], std=[0.34220656 0.33890437 0.19026635], max_rhat=21.576\n",
      "\u2713 MCMC Complete.\n",
      "\n",
      "Checking R-hat convergence...\n",
      "  \u2717 w_0       : max R-hat 149.107 (mean 40.771) > 1.05\n",
      "  \u2717 b_0       : max R-hat 121.110 (mean 60.120) > 1.05\n",
      "  \u2717 w_1       : max R-hat 45.666 (mean 22.323) > 1.05\n",
      "  \u2717 b_1       : max R-hat 49.217 (mean 28.308) > 1.05\n",
      "  \u2717 w_2       : max R-hat 185.080 (mean 45.672) > 1.05\n",
      "  \u2717 b_2       : max R-hat 21.576 (mean 18.030) > 1.05\n",
      "\n",
      "Overall max R-hat: 185.080\n",
      "\u2717 Convergence not achieved. Model NOT saved.\n",
      "  Recommendation: Increase num_warmup to 15000-20000\n",
      "\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "WARNING: Chains did not fully converge!\n",
      "\n",
      "Recommendations:\n",
      "  1. Increase num_warmup to 15000-20000\n",
      "  2. Run more chains (6-8) for better mixing\n",
      "  3. Try higher target_accept_prob (0.90-0.95)\n",
      "  4. Current settings are already very conservative\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "======================================================================\n",
      "FITTING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Pathfinder + NUTS MCMC Summary (Zero-Inflated NegBin):\n",
      "  - Likelihood: Zero-Inflated Negative Binomial\n",
      "  - Output scale: 25.65\n",
      "  - Network outputs: [zero_logit, log_mean, log_concentration]\n",
      "  - Pathfinder L-BFGS: ftol=1e-6, gtol=1e-9 (conservative)\n",
      "  - Initial states: Diverse samples from approximate posterior\n",
      "  - MCMC: 4 chains \u00d7 2000 samples with 10000 warmup\n",
      "  - MCMC step size: 1e-4 (conservative, will adapt during warmup)\n",
      "  - Total time: ~30-60 minutes first run, <1s cached\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Neural Model with ZERO-INFLATED NEGATIVE BINOMIAL Likelihood\n",
    "# 2 hidden layers of size 4\n",
    "# Network outputs 3 values: zero-inflation prob, mean, concentration\n",
    "\n",
    "# First, check the scale of the outcome\n",
    "print(f'Target variable (y) statistics:')\n",
    "print(f'  Min: {y_data.min():.2f}')\n",
    "print(f'  Max: {y_data.max():.2f}')\n",
    "print(f'  Mean: {y_data.mean():.2f}')\n",
    "print(f'  Std: {y_data.std():.2f}')\n",
    "print(f'  Zeros: {(y_data == 0).sum()} / {len(y_data)} ({100*(y_data == 0).mean():.1f}%)')\n",
    "if (y_data > 0).any():\n",
    "    print(f'  y[y>0] mean: {y_data[y_data > 0].mean():.2f}')\n",
    "\n",
    "# Use the mean as the output scale\n",
    "output_mean = float(y_data.mean())\n",
    "\n",
    "model = NeuralNegativeBinomialRegression(\n",
    "    dim_regressors=X_data.shape[1],\n",
    "    hidden_size=4,\n",
    "    depth=2,\n",
    "    output_scale=output_mean,  # Scale the mean by y.mean()\n",
    "    zero_inflated=True,  # Use zero-inflated model\n",
    "    dtype=jnp.float32\n",
    ")\n",
    "\n",
    "print(f'\\nModel: NeuralNegativeBinomialRegression (Zero-Inflated)')\n",
    "print(f'  - Likelihood: Zero-Inflated Negative Binomial')\n",
    "print(f'  - Output scale: {output_mean:.2f} (mean of y)')\n",
    "print(f'  - Network outputs: 3 (zero_logit, log_mean, log_concentration)')\n",
    "print(f'  - Hidden layers: 2 \u00d7 {4} neurons')\n",
    "print(f'  - Activation: ReLU')\n",
    "print(f'  - Handles overdispersion and excess zeros')\n",
    "\n",
    "# Data dictionary\n",
    "data_dict = {'X': X_data, 'y': y_data}\n",
    "data_jax = {'X': jnp.array(X_data), 'y': jnp.array(y_data)}\n",
    "\n",
    "# Check init\n",
    "print('\\nChecking initialization...')\n",
    "try:\n",
    "    seed = jax.random.PRNGKey(42)\n",
    "    params = model.prior_distribution.sample(2, seed=seed)\n",
    "    print('Initial sample keys:', params.keys())\n",
    "except Exception as e:\n",
    "    print(f'Initialization failed: {e}')\n",
    "\n",
    "# ============================================================================\n",
    "# PATHFINDER INITIALIZATION FOR MCMC\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('PATHFINDER INITIALIZATION STRATEGY (Conservative Settings)')\n",
    "print('='*70)\n",
    "\n",
    "import jax.flatten_util\n",
    "import blackjax.vi.pathfinder as pathfinder\n",
    "\n",
    "def pathfinder_initialization(model, data, num_chains=4, num_samples=200, maxiter=100, \n",
    "                              ftol=1e-6, gtol=1e-9, verbose=True):\n",
    "    \"\"\"\n",
    "    Use Pathfinder variational inference to initialize MCMC chains.\n",
    "    \n",
    "    Pathfinder (Zhang et al. 2022) is state-of-the-art for MCMC initialization:\n",
    "    - Finds posterior mode using L-BFGS (quasi-Newton method with line search)\n",
    "    - Builds multivariate normal approximation using inverse Hessian\n",
    "    - Importance samples to get diverse, high-quality initial states\n",
    "    \n",
    "    Args:\n",
    "        model: BayesianModel instance\n",
    "        data: Data dictionary\n",
    "        num_chains: Number of MCMC chains to initialize\n",
    "        num_samples: Importance samples (higher = better diversity, default 200)\n",
    "        maxiter: Max L-BFGS iterations (default 100)\n",
    "        ftol: Function tolerance for L-BFGS (smaller = more conservative, default 1e-6)\n",
    "        gtol: Gradient tolerance for L-BFGS (smaller = more conservative, default 1e-9)\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        initial_states: Dict[str, Array] with shape (num_chains, ...)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('\\nStep 1: Running Pathfinder variational inference...')\n",
    "        print(f'  L-BFGS settings: ftol={ftol}, gtol={gtol} (conservative)')\n",
    "    \n",
    "    # Setup parameter flattening\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    prior_sample = model.prior_distribution.sample(1, seed=key)\n",
    "    template = {var: prior_sample[var][0] for var in model.var_list}\n",
    "    flat_template, unflatten_fn = jax.flatten_util.ravel_pytree(template)\n",
    "    param_dim = flat_template.shape[0]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'  Parameter space dimension: {param_dim}')\n",
    "    \n",
    "    # Define log probability for Pathfinder\n",
    "    def logprob_fn_flat(params_flat):\n",
    "        params_dict = unflatten_fn(params_flat)\n",
    "        return model.unormalized_log_prob(data=data, **params_dict)\n",
    "    \n",
    "    # Run Pathfinder with conservative tolerances\n",
    "    initial_position = jax.random.normal(jax.random.PRNGKey(42), (param_dim,)) * 0.1\n",
    "    \n",
    "    state, info = pathfinder.approximate(\n",
    "        rng_key=jax.random.PRNGKey(123),\n",
    "        logdensity_fn=logprob_fn_flat,\n",
    "        initial_position=initial_position,\n",
    "        num_samples=num_samples,\n",
    "        maxiter=maxiter,\n",
    "        ftol=ftol,  # Conservative function tolerance\n",
    "        gtol=gtol,  # Conservative gradient tolerance\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'  \u2713 Pathfinder converged! ELBO: {float(state.elbo):.3f}')\n",
    "    \n",
    "    # Sample diverse initial states\n",
    "    if verbose:\n",
    "        print(f'\\nStep 2: Sampling {num_chains} diverse initial states...')\n",
    "    \n",
    "    sample_key = jax.random.PRNGKey(456)\n",
    "    samples_result = pathfinder.sample(sample_key, state, num_samples=num_chains)\n",
    "    samples_flat = samples_result[0] if isinstance(samples_result, tuple) else samples_result\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'  \u2713 Sampled from approximate posterior')\n",
    "    \n",
    "    # Unflatten and organize by parameter\n",
    "    chain_inits = {var: [] for var in model.var_list}\n",
    "    \n",
    "    for i in range(num_chains):\n",
    "        sample_dict = unflatten_fn(samples_flat[i])\n",
    "        for var_name in model.var_list:\n",
    "            chain_inits[var_name].append(sample_dict[var_name])\n",
    "    \n",
    "    # Stack into (num_chains, ...) format\n",
    "    for var_name in model.var_list:\n",
    "        chain_inits[var_name] = jnp.stack(chain_inits[var_name], axis=0)\n",
    "        if verbose:\n",
    "            print(f'  {var_name}: shape {chain_inits[var_name].shape}')\n",
    "    \n",
    "    if verbose:\n",
    "        print('  \u2713 Initial states ready for MCMC')\n",
    "    \n",
    "    return chain_inits\n",
    "\n",
    "def check_rhat_and_save(model, cache_dir, threshold=1.05):\n",
    "    \"\"\"Check R-hat convergence with strict threshold.\"\"\"\n",
    "    import tensorflow_probability.substrates.jax.mcmc as tfmcmc\n",
    "    print(\"\\nChecking R-hat convergence...\")\n",
    "    all_good = True\n",
    "    max_rhat_overall = 0.0\n",
    "    \n",
    "    for var, samples in model.mcmc_samples.items():\n",
    "        samples_transposed = jnp.swapaxes(samples, 0, 1)\n",
    "        rhat = tfmcmc.potential_scale_reduction(samples_transposed)\n",
    "        rhat = jnp.where(jnp.isnan(rhat), 1.0, rhat)\n",
    "        max_r = float(jnp.max(rhat))\n",
    "        mean_r = float(jnp.mean(rhat))\n",
    "        max_rhat_overall = max(max_rhat_overall, max_r)\n",
    "        \n",
    "        if max_r > threshold:\n",
    "            all_good = False\n",
    "            print(f\"  \u2717 {var:10s}: max R-hat {max_r:.3f} (mean {mean_r:.3f}) > {threshold}\")\n",
    "        else:\n",
    "            print(f\"  \u2713 {var:10s}: max R-hat {max_r:.3f} (mean {mean_r:.3f})\")\n",
    "    \n",
    "    print(f\"\\nOverall max R-hat: {max_rhat_overall:.3f}\")\n",
    "    \n",
    "    if all_good:\n",
    "        print(f\"\u2713 EXCELLENT: All R-hat < {threshold}! Saving model to {cache_dir}...\")\n",
    "        model.save_to_disk(cache_dir)\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"\u2717 Convergence not achieved. Model NOT saved.\")\n",
    "        print(f\"  Recommendation: Increase num_warmup to 15000-20000\")\n",
    "        return False\n",
    "\n",
    "# Main fitting logic with Pathfinder\n",
    "cache_dir = '/tmp/neural_regression_zinb'\n",
    "\n",
    "if os.path.exists(os.path.join(cache_dir, 'config.yaml')):\n",
    "    print(f\"\\nLoading fitted model from {cache_dir}...\")\n",
    "    try:\n",
    "        model = NeuralNegativeBinomialRegression.load_from_disk(cache_dir)\n",
    "        print(\"\u2713 Model loaded from cache!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u2717 Failed to load model: {e}\")\n",
    "        print(\"  Will refit with Pathfinder...\")\n",
    "        \n",
    "        # Run Pathfinder with conservative settings\n",
    "        chain_inits = pathfinder_initialization(\n",
    "            model, data_dict, \n",
    "            num_chains=4, \n",
    "            num_samples=200,\n",
    "            maxiter=100,\n",
    "            ftol=1e-6,  # Conservative L-BFGS tolerance\n",
    "            gtol=1e-9   # Conservative gradient tolerance\n",
    "        )\n",
    "        \n",
    "        # Run MCMC with conservative settings\n",
    "        print('\\nStep 3: Running MCMC with Pathfinder initialization...')\n",
    "        try:\n",
    "            model.fit_mcmc(\n",
    "                data=data_dict,\n",
    "                num_samples=2000,\n",
    "                num_warmup=10000,\n",
    "                num_chains=4,\n",
    "                target_accept_prob=0.85,\n",
    "                step_size=1e-4,  # Conservative initial step size\n",
    "                initial_states=chain_inits\n",
    "            )\n",
    "            print(\"\u2713 MCMC Complete.\")\n",
    "            check_rhat_and_save(model, cache_dir, threshold=1.05)\n",
    "        except Exception as e:\n",
    "            print(f\"\u2717 MCMC failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "else:\n",
    "    print('\\nNo cached model found. Starting fresh fit with Pathfinder...')\n",
    "    \n",
    "    # Run Pathfinder initialization with conservative settings\n",
    "    chain_inits = pathfinder_initialization(\n",
    "        model, data_dict,\n",
    "        num_chains=4,\n",
    "        num_samples=200,    # More importance samples = better diversity\n",
    "        maxiter=100,        # More iterations = better convergence\n",
    "        ftol=1e-6,          # Conservative function tolerance for L-BFGS\n",
    "        gtol=1e-9           # Conservative gradient tolerance for L-BFGS\n",
    "    )\n",
    "    \n",
    "    # Run MCMC with Pathfinder initialization and conservative settings\n",
    "    print('\\nStep 3: Running MCMC with Pathfinder-initialized chains...')\n",
    "    try:\n",
    "        model.fit_mcmc(\n",
    "            data=data_dict,\n",
    "            num_samples=2000,       # Post-warmup samples\n",
    "            num_warmup=10000,       # Long warmup for adaptation\n",
    "            num_chains=4,           # 4 chains for convergence detection\n",
    "            target_accept_prob=0.85,# Higher for difficult posteriors\n",
    "            step_size=1e-4,         # Conservative initial step size (will adapt)\n",
    "            initial_states=chain_inits  # Pathfinder initialization!\n",
    "        )\n",
    "        print(\"\u2713 MCMC Complete.\")\n",
    "        \n",
    "        # Check convergence\n",
    "        converged = check_rhat_and_save(model, cache_dir, threshold=1.05)\n",
    "        \n",
    "        if not converged:\n",
    "            print(\"\\n\" + \"!\"*70)\n",
    "            print(\"WARNING: Chains did not fully converge!\")\n",
    "            print(\"\\nRecommendations:\")\n",
    "            print(\"  1. Increase num_warmup to 15000-20000\")\n",
    "            print(\"  2. Run more chains (6-8) for better mixing\")\n",
    "            print(\"  3. Try higher target_accept_prob (0.90-0.95)\")\n",
    "            print(\"  4. Current settings are already very conservative\")\n",
    "            print(\"!\"*70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u2717 MCMC failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('FITTING COMPLETE')\n",
    "print('='*70)\n",
    "print('\\nPathfinder + NUTS MCMC Summary (Zero-Inflated NegBin):')\n",
    "print(f'  - Likelihood: Zero-Inflated Negative Binomial')\n",
    "print(f'  - Output scale: {output_mean:.2f}')\n",
    "print(f'  - Network outputs: [zero_logit, log_mean, log_concentration]')\n",
    "print('  - Pathfinder L-BFGS: ftol=1e-6, gtol=1e-9 (conservative)')\n",
    "print('  - Initial states: Diverse samples from approximate posterior')\n",
    "print('  - MCMC: 4 chains \u00d7 2000 samples with 10000 warmup')\n",
    "print('  - MCMC step size: 1e-4 (conservative, will adapt during warmup)')\n",
    "print('  - Total time: ~30-60 minutes first run, <1s cached')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "zlaszekt81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 100 simulations with s=1000 samples per run.\n",
      "Using ZERO-INFLATED NEGATIVE BINOMIAL likelihood\n",
      "  output_scale=25.65\n",
      "  zero_inflated=True\n",
      "Rhos: [4, 2, 1, 0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0078125, 0.00390625, 0.001953125, 0.0009765625]\n",
      "Base Transform: ['identity']\n",
      "Other Transforms: ['ll', 'kl', 'var', 'mm1', 'mm2', 'pmm1', 'pmm2']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21ce73fe3f246028c2adb92b3aab0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Simulations:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim 1: Found 262 / 262 data points needing adaptation.\n",
      "Sim 2: Found 262 / 262 data points needing adaptation.\n",
      "Sim 3: Found 262 / 262 data points needing adaptation.\n",
      "Sim 4: Found 262 / 262 data points needing adaptation.\n",
      "Sim 5: Found 262 / 262 data points needing adaptation.\n",
      "Sim 6: Found 262 / 262 data points needing adaptation.\n",
      "Sim 7: Found 262 / 262 data points needing adaptation.\n",
      "Sim 8: Found 262 / 262 data points needing adaptation.\n",
      "Sim 9: Found 262 / 262 data points needing adaptation.\n",
      "Sim 10: Found 262 / 262 data points needing adaptation.\n",
      "Sim 11: Found 262 / 262 data points needing adaptation.\n",
      "Sim 12: Found 262 / 262 data points needing adaptation.\n",
      "Sim 13: Found 262 / 262 data points needing adaptation.\n",
      "Sim 14: Found 262 / 262 data points needing adaptation.\n",
      "Sim 15: Found 262 / 262 data points needing adaptation.\n",
      "Sim 16: Found 262 / 262 data points needing adaptation.\n",
      "Sim 17: Found 262 / 262 data points needing adaptation.\n",
      "Sim 18: Found 262 / 262 data points needing adaptation.\n",
      "Sim 19: Found 262 / 262 data points needing adaptation.\n",
      "Sim 20: Found 262 / 262 data points needing adaptation.\n",
      "Sim 21: Found 262 / 262 data points needing adaptation.\n",
      "Sim 22: Found 262 / 262 data points needing adaptation.\n",
      "Sim 23: Found 262 / 262 data points needing adaptation.\n",
      "Sim 24: Found 262 / 262 data points needing adaptation.\n",
      "Sim 25: Found 262 / 262 data points needing adaptation.\n",
      "Sim 26: Found 262 / 262 data points needing adaptation.\n",
      "Sim 27: Found 262 / 262 data points needing adaptation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     75\u001b[39m data_subset = {\n\u001b[32m     76\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mX\u001b[39m\u001b[33m'\u001b[39m: data_jax[\u001b[33m'\u001b[39m\u001b[33mX\u001b[39m\u001b[33m'\u001b[39m][batch_idx],\n\u001b[32m     77\u001b[39m     \u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m: data_jax[\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m][batch_idx]\n\u001b[32m     78\u001b[39m }\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Run AIS on subset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m results_subset = \u001b[43mais_sampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43madaptive_is_loo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrhos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrhos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariational\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mother_transforms\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Update method stats\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m base_method \u001b[38;5;129;01min\u001b[39;00m other_transforms:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Find best khat for this base method (across rhos)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/bayesianquilts/metrics/ais.py:242\u001b[39m, in \u001b[36mAdaptiveImportanceSampler.adaptive_is_loo\u001b[39m\u001b[34m(self, data, params, hbar, rhos, variational, transformations)\u001b[39m\n\u001b[32m    238\u001b[39m      key = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_rho\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrho\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# Call transformation\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# Note: kwargs are passed. PMM1/PMM2 use 'hbar'. LL/KL/Var use 'hbar'.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m res = \u001b[43mtransform_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_ell\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_ell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_ell_prime\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_ell_prime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_ell_doubleprime\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_ell_doubleprime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtheta_std\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtheta_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariational\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariational\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_pi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_pi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_log_pi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_log_pi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_ell_original\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_ell\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# Check for NaNs in khat\u001b[39;00m\n\u001b[32m    258\u001b[39m khat_safe = jnp.where(jnp.isnan(res[\u001b[33m'\u001b[39m\u001b[33mkhat\u001b[39m\u001b[33m'\u001b[39m]), jnp.inf, res[\u001b[33m'\u001b[39m\u001b[33mkhat\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/bayesianquilts/metrics/ais.py:593\u001b[39m, in \u001b[36mAdaptiveImportanceSampler._transform_variance_based\u001b[39m\u001b[34m(self, data, params, theta, log_ell, log_ell_prime, log_ell_doubleprime, theta_std, hbar, variational, log_pi, grad_log_pi, log_ell_original, **kwargs)\u001b[39m\n\u001b[32m    584\u001b[39m             params_new[key] = jnp.concatenate([\n\u001b[32m    585\u001b[39m                 params_new[key],\n\u001b[32m    586\u001b[39m                 params_new_i[key][:, jnp.newaxis, ...]\n\u001b[32m    587\u001b[39m             ], axis=\u001b[32m1\u001b[39m)\n\u001b[32m    589\u001b[39m eta_weights, psis_weights, khat, log_ell_new = \u001b[38;5;28mself\u001b[39m._compute_importance_weights(\n\u001b[32m    590\u001b[39m     data, params, params_new, log_jacobian, variational, log_pi, log_ell_original=log_ell\n\u001b[32m    591\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlikelihood_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m weight_entropy = \u001b[38;5;28mself\u001b[39m.entropy(eta_weights)\n\u001b[32m    595\u001b[39m psis_entropy = \u001b[38;5;28mself\u001b[39m.entropy(psis_weights)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/bayesianquilts/predictors/nn/negbin.py:182\u001b[39m, in \u001b[36mNeuralNegativeBinomialLikelihood.log_likelihood\u001b[39m\u001b[34m(self, data, params)\u001b[39m\n\u001b[32m    179\u001b[39m          \u001b[38;5;28;01mreturn\u001b[39;00m jnp.squeeze(ll)\n\u001b[32m    181\u001b[39m      in_axes_params = jax.tree_util.tree_map(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[32m1\u001b[39m, params)\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m      ll_val = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_data_ll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m      \u001b[38;5;28;01mreturn\u001b[39;00m jnp.swapaxes(ll_val, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/api.py:1208\u001b[39m, in \u001b[36mvmap.<locals>.vmap_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1205\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1206\u001b[39m   axis_data = batching.AxisData(axis_name, axis_size_, spmd_axis_name,\n\u001b[32m   1207\u001b[39m                                 explicit_mesh_axis)\n\u001b[32m-> \u001b[39m\u001b[32m1208\u001b[39m   out_flat = \u001b[43mbatching\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m      \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten_axes\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvmap out_axes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_axes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m batching.SpecMatchError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1213\u001b[39m   out_axes_flat = flatten_axes(\u001b[33m\"\u001b[39m\u001b[33mvmap out_axes\u001b[39m\u001b[33m\"\u001b[39m, out_tree(), out_axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/linear_util.py:212\u001b[39m, in \u001b[36mWrappedFun.call_wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    211\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls the transformed function\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf_transformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/interpreters/batching.py:373\u001b[39m, in \u001b[36m_batch_outer\u001b[39m\u001b[34m(f, axis_data, in_dims, *in_vals)\u001b[39m\n\u001b[32m    371\u001b[39m tag = TraceTag()\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m source_info_util.transform_name_stack(\u001b[33m'\u001b[39m\u001b[33mvmap\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m   outs, trace = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43min_vals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m core.ensure_no_leaks(trace): \u001b[38;5;28;01mdel\u001b[39;00m trace\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/interpreters/batching.py:392\u001b[39m, in \u001b[36m_batch_inner\u001b[39m\u001b[34m(f, axis_data, out_dim_dests, tag, in_dims, *in_vals)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;66;03m# TODO(yashkatariya): Instead of `add_explicit_mesh_axis_names`, we should\u001b[39;00m\n\u001b[32m    387\u001b[39m \u001b[38;5;66;03m# create a new mesh by removing the axis_data.explicit_mesh_axis from it.\u001b[39;00m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (core.set_current_trace(trace),\n\u001b[32m    389\u001b[39m       core.extend_axis_env_nd([(axis_data.name, axis_data.size)]),\n\u001b[32m    390\u001b[39m       core.add_spmd_axis_names(axis_data.spmd_name),\n\u001b[32m    391\u001b[39m       core.add_explicit_mesh_axis_names(axis_data.explicit_mesh_axis)):\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m   outs = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43min_tracers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    393\u001b[39m   out_dim_dests = out_dim_dests() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(out_dim_dests) \u001b[38;5;28;01melse\u001b[39;00m out_dim_dests\n\u001b[32m    394\u001b[39m   out_vals = \u001b[38;5;28mmap\u001b[39m(partial(from_elt, trace, axis_data.size, axis_data.explicit_mesh_axis),\n\u001b[32m    395\u001b[39m                  \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(outs)), outs, out_dim_dests)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/interpreters/batching.py:128\u001b[39m, in \u001b[36mflatten_fun_for_vmap\u001b[39m\u001b[34m(f, store, in_tree, *args_flat)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;129m@lu\u001b[39m.transformation_with_aux2\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mflatten_fun_for_vmap\u001b[39m(f: Callable,\n\u001b[32m    126\u001b[39m                          store: lu.Store, in_tree: PyTreeDef, *args_flat):\n\u001b[32m    127\u001b[39m   py_args, py_kwargs = tree_unflatten(in_tree, args_flat)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m   ans = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m   ans, out_tree = tree_flatten(ans, is_leaf=is_vmappable)\n\u001b[32m    130\u001b[39m   store.store(out_tree)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/bayesianquilts/predictors/nn/negbin.py:178\u001b[39m, in \u001b[36mNeuralNegativeBinomialLikelihood.log_likelihood.<locals>.single_data_ll\u001b[39m\u001b[34m(x_i, y_i, params_i)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msingle_data_ll\u001b[39m(x_i, y_i, params_i):\n\u001b[32m    177\u001b[39m     d = {\u001b[33m'\u001b[39m\u001b[33mX\u001b[39m\u001b[33m'\u001b[39m: x_i[\u001b[38;5;28;01mNone\u001b[39;00m, :], \u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m: y_i}\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     ll = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp.squeeze(ll)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/bayesianquilts/predictors/nn/negbin.py:148\u001b[39m, in \u001b[36mNeuralNegativeBinomialRegression.log_likelihood\u001b[39m\u001b[34m(self, data, **params)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_likelihood\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, **params):\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictive_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mlog_likelihood\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/bayesianquilts/predictors/nn/negbin.py:109\u001b[39m, in \u001b[36mNeuralNegativeBinomialRegression.predictive_distribution\u001b[39m\u001b[34m(self, data, **params)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.zero_inflated:\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# Zero-inflated negative binomial log-likelihood\u001b[39;00m\n\u001b[32m    105\u001b[39m     \u001b[38;5;66;03m# P(Y=0) = zero_prob + (1 - zero_prob) * NB(0)\u001b[39;00m\n\u001b[32m    106\u001b[39m     \u001b[38;5;66;03m# P(Y>0) = (1 - zero_prob) * NB(y)\u001b[39;00m\n\u001b[32m    108\u001b[39m     nb_dist = tfd.NegativeBinomial(total_count=total_count, probs=probs)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     nb_logprob = \u001b[43mnb_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     \u001b[38;5;66;03m# For y=0: log(zero_prob + (1-zero_prob)*exp(nb_logprob(0)))\u001b[39;00m\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# For y>0: log(1-zero_prob) + nb_logprob(y)\u001b[39;00m\n\u001b[32m    113\u001b[39m     nb_logprob_zero = nb_dist.log_prob(jnp.zeros_like(y))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1287\u001b[39m, in \u001b[36mDistribution.log_prob\u001b[39m\u001b[34m(self, value, name, **kwargs)\u001b[39m\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value, name=\u001b[33m'\u001b[39m\u001b[33mlog_prob\u001b[39m\u001b[33m'\u001b[39m, **kwargs):\n\u001b[32m   1276\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Log probability density/mass function.\u001b[39;00m\n\u001b[32m   1277\u001b[39m \n\u001b[32m   1278\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1285\u001b[39m \u001b[33;03m      values of type `self.dtype`.\u001b[39;00m\n\u001b[32m   1286\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1287\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1269\u001b[39m, in \u001b[36mDistribution._call_log_prob\u001b[39m\u001b[34m(self, value, name, **kwargs)\u001b[39m\n\u001b[32m   1267\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._name_and_control_scope(name, value, kwargs):\n\u001b[32m   1268\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_log_prob\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1269\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1270\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_prob\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1271\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.math.log(\u001b[38;5;28mself\u001b[39m._prob(value, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/distributions/negative_binomial.py:215\u001b[39m, in \u001b[36mNegativeBinomial._log_prob\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    211\u001b[39m logits = \u001b[38;5;28mself\u001b[39m._logits_parameter_no_checks()\n\u001b[32m    212\u001b[39m log_unnormalized_prob = (\n\u001b[32m    213\u001b[39m     total_count * tf.math.log_sigmoid(-logits) +\n\u001b[32m    214\u001b[39m     tf.math.multiply_no_nan(tf.math.log_sigmoid(logits), x))\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m log_normalization = (\u001b[43mspecial\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlbeta\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_count\u001b[49m\u001b[43m)\u001b[49m +\n\u001b[32m    216\u001b[39m                      tf.math.log(total_count + x))\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m log_unnormalized_prob - log_normalization\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/math/special.py:2128\u001b[39m, in \u001b[36mlbeta\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m   2126\u001b[39m x = tf.convert_to_tensor(x, dtype=dtype)\n\u001b[32m   2127\u001b[39m y = tf.convert_to_tensor(y, dtype=dtype)\n\u001b[32m-> \u001b[39m\u001b[32m2128\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lbeta_custom_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/custom_derivatives.py:303\u001b[39m, in \u001b[36mcustom_jvp.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    300\u001b[39m flat_fun, out_type1 = _flatten_fun_nokwargs(f_, in_tree)\n\u001b[32m    301\u001b[39m flat_jvp, out_type2 = _flatten_jvp(jvp, primal_name, debug_jvp.func_name,\n\u001b[32m    302\u001b[39m                                    in_tree, out_type1)\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m out_flat = \u001b[43mcustom_jvp_call_p\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_jvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43msymbolic_zeros\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msymbolic_zeros\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m _, (out_tree, _, _) = lu.merge_linear_aux(out_type1, out_type2)\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(out_tree, out_flat)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/custom_derivatives.py:390\u001b[39m, in \u001b[36mCustomJVPCallPrimitive.bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **params):\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_true_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/core.py:649\u001b[39m, in \u001b[36mPrimitive._true_bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    647\u001b[39m trace_ctx.set_trace(eval_trace)\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    651\u001b[39m   trace_ctx.set_trace(prev_trace)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/custom_derivatives.py:394\u001b[39m, in \u001b[36mCustomJVPCallPrimitive.bind_with_trace\u001b[39m\u001b[34m(self, trace, args, params)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[32m    393\u001b[39m   fun, jvp, tracers = args[\u001b[32m0\u001b[39m], args[\u001b[32m1\u001b[39m], args[\u001b[32m2\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_custom_jvp_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/interpreters/batching.py:329\u001b[39m, in \u001b[36mBatchTrace.process_custom_jvp_call\u001b[39m\u001b[34m(self, prim, fun, jvp, tracers, symbolic_zeros)\u001b[39m\n\u001b[32m    327\u001b[39m fun, out_dims1 = batch_subtrace(fun, \u001b[38;5;28mself\u001b[39m.tag, \u001b[38;5;28mself\u001b[39m.axis_data, in_dims)\n\u001b[32m    328\u001b[39m jvp, out_dims2 = batch_custom_jvp_subtrace(jvp, \u001b[38;5;28mself\u001b[39m.tag, \u001b[38;5;28mself\u001b[39m.axis_data, in_dims)\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m out_vals = \u001b[43mprim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43min_vals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msymbolic_zeros\u001b[49m\u001b[43m=\u001b[49m\u001b[43msymbolic_zeros\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m fst, out_dims = lu.merge_linear_aux(out_dims1, out_dims2)\n\u001b[32m    332\u001b[39m src = source_info_util.current()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/custom_derivatives.py:394\u001b[39m, in \u001b[36mCustomJVPCallPrimitive.bind_with_trace\u001b[39m\u001b[34m(self, trace, args, params)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[32m    393\u001b[39m   fun, jvp, tracers = args[\u001b[32m0\u001b[39m], args[\u001b[32m1\u001b[39m], args[\u001b[32m2\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_custom_jvp_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/core.py:1227\u001b[39m, in \u001b[36mEvalTrace.process_custom_jvp_call\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_custom_jvp_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, fun, jvp, tracers, **_):\n\u001b[32m   1226\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m primitive, jvp, _  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1227\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtracers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/linear_util.py:212\u001b[39m, in \u001b[36mWrappedFun.call_wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    211\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls the transformed function\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf_transformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/interpreters/batching.py:440\u001b[39m, in \u001b[36mbatch_subtrace\u001b[39m\u001b[34m(f, store, tag, axis_data, in_dims, *in_vals)\u001b[39m\n\u001b[32m    437\u001b[39m     in_dims = in_dims() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(in_dims) \u001b[38;5;28;01melse\u001b[39;00m in_dims\n\u001b[32m    438\u001b[39m     in_tracers = [BatchTracer(trace, x, dim, source_info_util.current())\n\u001b[32m    439\u001b[39m                   \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x, dim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(in_vals, in_dims)]\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     outs = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43min_tracers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m   out_vals, out_dims = unzip2(\u001b[38;5;28mmap\u001b[39m(trace.to_batch_info, outs))\n\u001b[32m    442\u001b[39m store.store(out_dims)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/custom_derivatives.py:87\u001b[39m, in \u001b[36m_flatten_fun_nokwargs\u001b[39m\u001b[34m(f, store, in_tree, *args_flat)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;129m@lu\u001b[39m.transformation_with_aux2\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_flatten_fun_nokwargs\u001b[39m(f: Callable,\n\u001b[32m     84\u001b[39m                           store: lu.Store, in_tree: PyTreeDef,\n\u001b[32m     85\u001b[39m                           *args_flat):\n\u001b[32m     86\u001b[39m   py_args = tree_unflatten(in_tree, args_flat)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m   ans = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m   ans_flat, ans_tree = tree_flatten(ans)\n\u001b[32m     89\u001b[39m   ans_avals = [core.get_aval(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ans_flat]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/math/special.py:2088\u001b[39m, in \u001b[36m_lbeta_custom_gradient\u001b[39m\u001b[34m(x, y)\u001b[39m\n\u001b[32m   2082\u001b[39m \u001b[38;5;129m@tfp_custom_gradient\u001b[39m.custom_gradient(\n\u001b[32m   2083\u001b[39m     vjp_fwd=_lbeta_fwd,\n\u001b[32m   2084\u001b[39m     vjp_bwd=_lbeta_bwd,\n\u001b[32m   2085\u001b[39m     jvp_fn=_lbeta_jvp)\n\u001b[32m   2086\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_lbeta_custom_gradient\u001b[39m(x, y):\n\u001b[32m   2087\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Computes log(Beta(x, y)) with correct custom gradient.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2088\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lbeta_naive_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/math/special.py:2028\u001b[39m, in \u001b[36m_lbeta_naive_gradient\u001b[39m\u001b[34m(x, y)\u001b[39m\n\u001b[32m   2024\u001b[39m log2pi = tf.constant(np.log(\u001b[32m2\u001b[39m * np.pi), dtype=x.dtype)\n\u001b[32m   2025\u001b[39m \u001b[38;5;66;03m# Two large arguments case: y >= x >= 8.\u001b[39;00m\n\u001b[32m   2026\u001b[39m log_beta_two_large = (half * log2pi\n\u001b[32m   2027\u001b[39m                       - half * tf.math.log(y)\n\u001b[32m-> \u001b[39m\u001b[32m2028\u001b[39m                       + \u001b[43mlog_gamma_correction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2029\u001b[39m                       + log_gamma_correction(y)\n\u001b[32m   2030\u001b[39m                       - log_gamma_correction(x + y)\n\u001b[32m   2031\u001b[39m                       + (x - half) * tf.math.log(x / (x + y))\n\u001b[32m   2032\u001b[39m                       - y * tf.math.log1p(x / y))\n\u001b[32m   2034\u001b[39m \u001b[38;5;66;03m# One large argument case: x < 8, y >= 8.\u001b[39;00m\n\u001b[32m   2035\u001b[39m log_beta_one_large = tf.math.lgamma(x) + _log_gamma_difference_big_y(x, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/math/special.py:1885\u001b[39m, in \u001b[36mlog_gamma_correction\u001b[39m\u001b[34m(x, name)\u001b[39m\n\u001b[32m   1854\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns the error of the Stirling approximation to lgamma(x) for x >= 8.\u001b[39;00m\n\u001b[32m   1855\u001b[39m \n\u001b[32m   1856\u001b[39m \u001b[33;03mThis is useful for accurately evaluating ratios between Gamma functions, as\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1882\u001b[39m \u001b[33;03m  lgamma_corr: Tensor of elementwise log gamma corrections.\u001b[39;00m\n\u001b[32m   1883\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1884\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.name_scope(name \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mlog_gamma_correction\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m   dtype = \u001b[43mdtype_util\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommon_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1886\u001b[39m   x = tf.convert_to_tensor(x, dtype=dtype)\n\u001b[32m   1888\u001b[39m   minimax_coeff = tf.constant([\n\u001b[32m   1889\u001b[39m       \u001b[32m0.833333333333333e-01\u001b[39m,\n\u001b[32m   1890\u001b[39m       -\u001b[32m0.277777777760991e-02\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1894\u001b[39m       -\u001b[32m0.165322962780713e-02\u001b[39m,\n\u001b[32m   1895\u001b[39m   ], dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/internal/dtype_util.py:189\u001b[39m, in \u001b[36mcommon_dtype\u001b[39m\u001b[34m(args, dtype_hint)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(flattened_args):\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(a, \u001b[33m'\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m a.dtype:\n\u001b[32m    188\u001b[39m     dt = tf.nest.map_structure(\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m d: d \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m as_numpy_dtype(d), \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m)\n\u001b[32m    190\u001b[39m     seen[i] = dt\n\u001b[32m    191\u001b[39m   \u001b[38;5;28;01melif\u001b[39;00m nest.is_nested(a):\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# `a` should have a shallow structure that matches `dtype_hint`, and each\u001b[39;00m\n\u001b[32m    193\u001b[39m     \u001b[38;5;66;03m# element of `a`'s shallow structure should have a single dtype (or None),\u001b[39;00m\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# which we obtain with a recursive call to `common_dtype`. We omit\u001b[39;00m\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# `dtype_hint` in this call because `dtype_hint` is applied to the entire\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;66;03m# nested dtype in the last line of the function.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/core.py:895\u001b[39m, in \u001b[36m_aval_property.<locals>.<lambda>\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    894\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_aval_property\u001b[39m(name):\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mproperty\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maval\u001b[49m, name))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/interpreters/batching.py:169\u001b[39m, in \u001b[36mBatchTracer.aval\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    167\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m aval\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.batch_dim) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmapped_aval\u001b[49m\u001b[43m(\u001b[49m\u001b[43maval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    171\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mbatch dim should be int or `not_mapped`\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/core.py:2969\u001b[39m, in \u001b[36mmapped_aval\u001b[39m\u001b[34m(size, axis, aval)\u001b[39m\n\u001b[32m   2967\u001b[39m handler, _ = aval_mapping_handlers.get(\u001b[38;5;28mtype\u001b[39m(aval), (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m   2968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2969\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2970\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2971\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mno mapping handler for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(aval)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/core.py:2990\u001b[39m, in \u001b[36m_map_shaped_array\u001b[39m\u001b[34m(size, axis, aval)\u001b[39m\n\u001b[32m   2987\u001b[39m aval_s = aval.sharding\n\u001b[32m   2988\u001b[39m sharding = aval_s.update(\n\u001b[32m   2989\u001b[39m     spec=aval_s.spec.update(partitions=tuple_delete(aval_s.spec, axis)))\n\u001b[32m-> \u001b[39m\u001b[32m2990\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mShapedArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtuple_delete\u001b[49m\u001b[43m(\u001b[49m\u001b[43maval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2991\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mweak_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43maval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweak_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvma\u001b[49m\u001b[43m=\u001b[49m\u001b[43maval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2992\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mmemory_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43maval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmemory_space\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/core.py:2194\u001b[39m, in \u001b[36mShapedArray.__init__\u001b[39m\u001b[34m(self, shape, dtype, weak_type, sharding, vma, memory_space)\u001b[39m\n\u001b[32m   2191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, shape, dtype, weak_type=\u001b[38;5;28;01mFalse\u001b[39;00m, *, sharding=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2192\u001b[39m              vma: \u001b[38;5;28mfrozenset\u001b[39m[AxisName] = \u001b[38;5;28mfrozenset\u001b[39m(),\n\u001b[32m   2193\u001b[39m              memory_space: MemorySpace = MemorySpace.Device):\n\u001b[32m-> \u001b[39m\u001b[32m2194\u001b[39m   \u001b[38;5;28mself\u001b[39m.shape = \u001b[43mcanonicalize_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2195\u001b[39m   \u001b[38;5;28mself\u001b[39m.dtype = _dtype_object(dtype)\n\u001b[32m   2196\u001b[39m   \u001b[38;5;28mself\u001b[39m.weak_type = weak_type\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/core.py:1982\u001b[39m, in \u001b[36mcanonicalize_shape\u001b[39m\u001b[34m(shape, context)\u001b[39m\n\u001b[32m   1980\u001b[39m   shape = shape,\n\u001b[32m   1981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1982\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(unsafe_map(_canonicalize_dimension, shape))\n\u001b[32m   1983\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1984\u001b[39m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/core.py:1962\u001b[39m, in \u001b[36m_canonicalize_dimension\u001b[39m\u001b[34m(dim)\u001b[39m\n\u001b[32m   1959\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_canonicalize_dimension\u001b[39m(dim: DimSize) -> DimSize:\n\u001b[32m   1960\u001b[39m   \u001b[38;5;66;03m# Dimensions are most commonly integral (by far), so we check that first.\u001b[39;00m\n\u001b[32m   1961\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1962\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m operator.index(dim)\n\u001b[32m   1963\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1964\u001b[39m     type_error = e\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Full Simulation for Table Metrics (Optimized for Memory)\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "# Initialize Likelihood and Sampler with ZERO-INFLATED NEGATIVE BINOMIAL likelihood\n",
    "likelihood_fn = NeuralNegativeBinomialLikelihood(model)\n",
    "ais_sampler = AdaptiveImportanceSampler(likelihood_fn=likelihood_fn)\n",
    "\n",
    "n_simulations = 100\n",
    "n_samples = 1000\n",
    "rhos = [ 2**-r for r in range(-2, 11) ]\n",
    "\n",
    "# Split transformations\n",
    "base_transform = ['identity']\n",
    "other_transforms = ['ll', 'kl', 'var', 'mm1', 'mm2', 'pmm1', 'pmm2']\n",
    "\n",
    "print(f\"Running {n_simulations} simulations with s={n_samples} samples per run.\")\n",
    "print(f\"Using ZERO-INFLATED NEGATIVE BINOMIAL likelihood\")\n",
    "print(f\"  output_scale={model.output_scale:.2f}\")\n",
    "print(f\"  zero_inflated={model.zero_inflated}\")\n",
    "print(f\"Rhos: {rhos}\")\n",
    "print(f\"Base Transform: {base_transform}\")\n",
    "print(f\"Other Transforms: {other_transforms}\")\n",
    "\n",
    "data_jax = {'X': jnp.array(X_data), 'y': jnp.array(y_data)}\n",
    "N_obs = data_jax['y'].shape[0]\n",
    "\n",
    "simulation_records = []\n",
    "\n",
    "for i in tqdm(range(n_simulations), desc=\"Simulations\"):\n",
    "    # 1. Sample Parameters\n",
    "    params = model.sample_mcmc(num_samples=n_samples)\n",
    "\n",
    "    # 2. Run AIS with Identity FIRST\n",
    "    results_base = ais_sampler.adaptive_is_loo(\n",
    "        data=data_jax,\n",
    "        params=params,\n",
    "        rhos=rhos,\n",
    "        variational=False, # MCMC samples\n",
    "        transformations=base_transform\n",
    "    )\n",
    "\n",
    "    # Extract identity khat (convert to numpy for mutability)\n",
    "    khat_identity = np.array(results_base['identity']['khat'])\n",
    "\n",
    "    # Identify problematic points\n",
    "    idx_bad = np.where(khat_identity >= 0.7)[0]\n",
    "\n",
    "    print(f\"Sim {i+1}: Found {len(idx_bad)} / {N_obs} data points needing adaptation.\")\n",
    "\n",
    "    # Clean up results_base immediately to free memory\n",
    "    del results_base\n",
    "    gc.collect()\n",
    "\n",
    "    # Prepare storage for this simulation's khats per method\n",
    "    # Initialize with identity khat for all (Hybrid approach: default to identity)\n",
    "    method_khats = {}\n",
    "    method_khats['identity'] = khat_identity\n",
    "    for m in other_transforms:\n",
    "        method_khats[m] = np.array(khat_identity.copy())\n",
    "\n",
    "    # 3. Process problematic points with other methods\n",
    "    if len(idx_bad) > 0:\n",
    "        batch_size = 16\n",
    "        num_batches = int(np.ceil(len(idx_bad) / batch_size))\n",
    "\n",
    "        for b in range(num_batches):\n",
    "            batch_idx = idx_bad[b*batch_size : (b+1)*batch_size]\n",
    "\n",
    "            # Create data subset for BAD points in this batch\n",
    "            # JAX arrays can be indexed by numpy arrays\n",
    "            data_subset = {\n",
    "                'X': data_jax['X'][batch_idx],\n",
    "                'y': data_jax['y'][batch_idx]\n",
    "            }\n",
    "\n",
    "            # Run AIS on subset\n",
    "            results_subset = ais_sampler.adaptive_is_loo(\n",
    "                data=data_subset,\n",
    "                params=params,\n",
    "                rhos=rhos,\n",
    "                variational=False,\n",
    "                transformations=other_transforms\n",
    "            )\n",
    "\n",
    "            # Update method stats\n",
    "            for base_method in other_transforms:\n",
    "                # Find best khat for this base method (across rhos)\n",
    "                khat_arrays = []\n",
    "                for key, res in results_subset.items():\n",
    "                    if key == 'best': continue\n",
    "                    if key == base_method or key.startswith(base_method + '_'):\n",
    "                        khat_arrays.append(res['khat'])\n",
    "\n",
    "                if khat_arrays:\n",
    "                    # Min over rhos for this method on the SUBSET\n",
    "                    min_khat_subset = np.array(np.min(np.stack(khat_arrays), axis=0))\n",
    "\n",
    "                    # Update the main array at the bad indices for this batch\n",
    "                    method_khats[base_method][batch_idx] = min_khat_subset\n",
    "\n",
    "            # Clean up batch results\n",
    "            del results_subset\n",
    "            gc.collect()\n",
    "\n",
    "    # Define Groups\n",
    "    groups = {\n",
    "        'Base': ['identity'],\n",
    "        'PMM1': ['pmm1'],\n",
    "        'PMM2': ['pmm2'],\n",
    "        'KL': ['kl'],\n",
    "        'Var': ['var'],\n",
    "        'Ours_Combined': ['pmm1', 'pmm2', 'kl', 'var'],\n",
    "        'LL': ['ll'],\n",
    "        'MM1': ['mm1'],\n",
    "        'MM2': ['mm2'],\n",
    "        'Full': other_transforms + base_transform\n",
    "    }\n",
    "\n",
    "    sim_counts = {}\n",
    "    for group_name, methods in groups.items():\n",
    "        grouped_khats = []\n",
    "        for m in methods:\n",
    "             if m in method_khats:\n",
    "                 grouped_khats.append(method_khats[m])\n",
    "\n",
    "        if grouped_khats:\n",
    "            # Best khat across ANY method in the group for each obs\n",
    "            # Take minimum khat (best adaptation)\n",
    "            best_group_khat = np.min(np.stack(grouped_khats), axis=0)\n",
    "            # Count FAILURES (khat > 0.7)\n",
    "            n_failures = np.sum(best_group_khat > 0.7)\n",
    "            sim_counts[group_name] = n_failures\n",
    "        else:\n",
    "            sim_counts[group_name] = np.nan\n",
    "\n",
    "    simulation_records.append(sim_counts)\n",
    "\n",
    "# 4. Aggregate Statistics across simulations\n",
    "df_sims = pd.DataFrame(simulation_records)\n",
    "stats = df_sims.agg(['mean', 'std'])\n",
    "\n",
    "print(\"\\n--- Table Metrics: Unsuccessful Adaptations (Roaches/ZINB) ---\")\n",
    "print(stats.round(1))\n",
    "\n",
    "# Optional: Format for LaTeX\n",
    "print(\"\\nLaTeX Format (Mean \u00b1 Std):\")\n",
    "for col in df_sims.columns:\n",
    "    m = stats.loc['mean', col]\n",
    "    s = stats.loc['std', col]\n",
    "    print(f\"{col}: {m:.1f} \u00b1 {s:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb2405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 100 simulations with s=1000 samples per run.\n",
      "Rhos: [4, 2, 1, 0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0078125, 0.00390625, 0.001953125, 0.0009765625]\n",
      "Base Transform: ['identity']\n",
      "Other Transforms: ['ll', 'kl', 'var', 'mm1', 'mm2', 'pmm1', 'pmm2']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57fae347d6e4dceb04e7327b6b553f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Simulations:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim 1: Found 259 / 262 data points needing adaptation.\n",
      "Sim 2: Found 258 / 262 data points needing adaptation.\n",
      "Sim 3: Found 259 / 262 data points needing adaptation.\n",
      "Sim 4: Found 261 / 262 data points needing adaptation.\n",
      "Sim 5: Found 258 / 262 data points needing adaptation.\n",
      "Sim 6: Found 259 / 262 data points needing adaptation.\n",
      "Sim 7: Found 261 / 262 data points needing adaptation.\n",
      "Sim 8: Found 259 / 262 data points needing adaptation.\n",
      "Sim 9: Found 260 / 262 data points needing adaptation.\n",
      "Sim 10: Found 261 / 262 data points needing adaptation.\n",
      "Sim 11: Found 260 / 262 data points needing adaptation.\n",
      "Sim 12: Found 261 / 262 data points needing adaptation.\n",
      "Sim 13: Found 261 / 262 data points needing adaptation.\n",
      "Sim 14: Found 260 / 262 data points needing adaptation.\n",
      "Sim 15: Found 260 / 262 data points needing adaptation.\n",
      "Sim 16: Found 259 / 262 data points needing adaptation.\n",
      "Sim 17: Found 261 / 262 data points needing adaptation.\n",
      "Sim 18: Found 261 / 262 data points needing adaptation.\n",
      "Sim 19: Found 261 / 262 data points needing adaptation.\n",
      "Sim 20: Found 256 / 262 data points needing adaptation.\n",
      "Sim 21: Found 260 / 262 data points needing adaptation.\n",
      "Sim 22: Found 261 / 262 data points needing adaptation.\n",
      "Sim 23: Found 259 / 262 data points needing adaptation.\n",
      "Sim 24: Found 261 / 262 data points needing adaptation.\n",
      "Sim 25: Found 259 / 262 data points needing adaptation.\n",
      "Sim 26: Found 260 / 262 data points needing adaptation.\n",
      "Sim 27: Found 261 / 262 data points needing adaptation.\n",
      "Sim 28: Found 260 / 262 data points needing adaptation.\n",
      "Sim 29: Found 258 / 262 data points needing adaptation.\n",
      "Sim 30: Found 261 / 262 data points needing adaptation.\n",
      "Sim 31: Found 262 / 262 data points needing adaptation.\n",
      "Sim 32: Found 260 / 262 data points needing adaptation.\n",
      "Sim 33: Found 260 / 262 data points needing adaptation.\n",
      "Sim 34: Found 261 / 262 data points needing adaptation.\n",
      "Sim 35: Found 260 / 262 data points needing adaptation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     72\u001b[39m data_subset = {\n\u001b[32m     73\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mX\u001b[39m\u001b[33m'\u001b[39m: data_jax[\u001b[33m'\u001b[39m\u001b[33mX\u001b[39m\u001b[33m'\u001b[39m][batch_idx],\n\u001b[32m     74\u001b[39m     \u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m: data_jax[\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m][batch_idx]\n\u001b[32m     75\u001b[39m }\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Run AIS on subset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m results_subset = \u001b[43mais_sampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43madaptive_is_loo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrhos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrhos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariational\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mother_transforms\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Update method stats\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m base_method \u001b[38;5;129;01min\u001b[39;00m other_transforms:\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# Find best khat for this base method (across rhos)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/bayesianquilts/metrics/ais.py:242\u001b[39m, in \u001b[36mAdaptiveImportanceSampler.adaptive_is_loo\u001b[39m\u001b[34m(self, data, params, hbar, rhos, variational, transformations)\u001b[39m\n\u001b[32m    238\u001b[39m      key = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_rho\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrho\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# Call transformation\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# Note: kwargs are passed. PMM1/PMM2 use 'hbar'. LL/KL/Var use 'hbar'.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m res = \u001b[43mtransform_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_ell\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_ell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_ell_prime\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_ell_prime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_ell_doubleprime\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_ell_doubleprime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtheta_std\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtheta_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariational\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariational\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_pi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_pi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_log_pi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_log_pi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_ell_original\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_ell\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# Check for NaNs in khat\u001b[39;00m\n\u001b[32m    258\u001b[39m khat_safe = jnp.where(jnp.isnan(res[\u001b[33m'\u001b[39m\u001b[33mkhat\u001b[39m\u001b[33m'\u001b[39m]), jnp.inf, res[\u001b[33m'\u001b[39m\u001b[33mkhat\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/bayesianquilts/metrics/ais.py:1008\u001b[39m, in \u001b[36mAdaptiveImportanceSampler._transform_pmm2\u001b[39m\u001b[34m(self, params, data, weight_fn, log_ell_original, hbar, variational, log_pi, **kwargs)\u001b[39m\n\u001b[32m   1003\u001b[39m eta_weights, psis_weights, khat, log_ell_new = \u001b[38;5;28mself\u001b[39m._compute_importance_weights(\n\u001b[32m   1004\u001b[39m     data=data, params_original=params, params_transformed=new_params, log_jacobian=log_jacobian, variational=variational, log_pi_original=log_pi, log_ell_original=log_ell_original\n\u001b[32m   1005\u001b[39m )\n\u001b[32m   1007\u001b[39m \u001b[38;5;66;03m# Compute predictions and diagnostics\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlikelihood_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1010\u001b[39m weight_entropy = \u001b[38;5;28mself\u001b[39m.entropy(eta_weights)\n\u001b[32m   1011\u001b[39m psis_entropy = \u001b[38;5;28mself\u001b[39m.entropy(psis_weights)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/bayesianquilts/predictors/nn/poisson.py:109\u001b[39m, in \u001b[36mNeuralPoissonLikelihood.log_likelihood\u001b[39m\u001b[34m(self, data, params)\u001b[39m\n\u001b[32m    106\u001b[39m in_axes_params = jax.tree_util.tree_map(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[32m1\u001b[39m, params)\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Result: (N, S)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m ll_val = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_data_ll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Return (S, N). Use swapaxes to be safe for high rank\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# If ll_val is (N, S), swaps to (S, N)\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jnp.swapaxes(ll_val, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/api.py:1208\u001b[39m, in \u001b[36mvmap.<locals>.vmap_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1205\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1206\u001b[39m   axis_data = batching.AxisData(axis_name, axis_size_, spmd_axis_name,\n\u001b[32m   1207\u001b[39m                                 explicit_mesh_axis)\n\u001b[32m-> \u001b[39m\u001b[32m1208\u001b[39m   out_flat = \u001b[43mbatching\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m      \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten_axes\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvmap out_axes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_axes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m batching.SpecMatchError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1213\u001b[39m   out_axes_flat = flatten_axes(\u001b[33m\"\u001b[39m\u001b[33mvmap out_axes\u001b[39m\u001b[33m\"\u001b[39m, out_tree(), out_axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/linear_util.py:212\u001b[39m, in \u001b[36mWrappedFun.call_wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    211\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls the transformed function\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf_transformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/interpreters/batching.py:373\u001b[39m, in \u001b[36m_batch_outer\u001b[39m\u001b[34m(f, axis_data, in_dims, *in_vals)\u001b[39m\n\u001b[32m    371\u001b[39m tag = TraceTag()\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m source_info_util.transform_name_stack(\u001b[33m'\u001b[39m\u001b[33mvmap\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m   outs, trace = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43min_vals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m core.ensure_no_leaks(trace): \u001b[38;5;28;01mdel\u001b[39;00m trace\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/interpreters/batching.py:392\u001b[39m, in \u001b[36m_batch_inner\u001b[39m\u001b[34m(f, axis_data, out_dim_dests, tag, in_dims, *in_vals)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;66;03m# TODO(yashkatariya): Instead of `add_explicit_mesh_axis_names`, we should\u001b[39;00m\n\u001b[32m    387\u001b[39m \u001b[38;5;66;03m# create a new mesh by removing the axis_data.explicit_mesh_axis from it.\u001b[39;00m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (core.set_current_trace(trace),\n\u001b[32m    389\u001b[39m       core.extend_axis_env_nd([(axis_data.name, axis_data.size)]),\n\u001b[32m    390\u001b[39m       core.add_spmd_axis_names(axis_data.spmd_name),\n\u001b[32m    391\u001b[39m       core.add_explicit_mesh_axis_names(axis_data.explicit_mesh_axis)):\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m   outs = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43min_tracers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    393\u001b[39m   out_dim_dests = out_dim_dests() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(out_dim_dests) \u001b[38;5;28;01melse\u001b[39;00m out_dim_dests\n\u001b[32m    394\u001b[39m   out_vals = \u001b[38;5;28mmap\u001b[39m(partial(from_elt, trace, axis_data.size, axis_data.explicit_mesh_axis),\n\u001b[32m    395\u001b[39m                  \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(outs)), outs, out_dim_dests)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/interpreters/batching.py:128\u001b[39m, in \u001b[36mflatten_fun_for_vmap\u001b[39m\u001b[34m(f, store, in_tree, *args_flat)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;129m@lu\u001b[39m.transformation_with_aux2\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mflatten_fun_for_vmap\u001b[39m(f: Callable,\n\u001b[32m    126\u001b[39m                          store: lu.Store, in_tree: PyTreeDef, *args_flat):\n\u001b[32m    127\u001b[39m   py_args, py_kwargs = tree_unflatten(in_tree, args_flat)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m   ans = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m   ans, out_tree = tree_flatten(ans, is_leaf=is_vmappable)\n\u001b[32m    130\u001b[39m   store.store(out_tree)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/bayesianquilts/predictors/nn/poisson.py:101\u001b[39m, in \u001b[36mNeuralPoissonLikelihood.log_likelihood.<locals>.single_data_ll\u001b[39m\u001b[34m(x_i, y_i, params_i)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msingle_data_ll\u001b[39m(x_i, y_i, params_i):\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# params_i: (S, D, H)\u001b[39;00m\n\u001b[32m    100\u001b[39m     d = {\u001b[33m'\u001b[39m\u001b[33mX\u001b[39m\u001b[33m'\u001b[39m: x_i[\u001b[38;5;28;01mNone\u001b[39;00m, :], \u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m: y_i} \n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     ll = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     \u001b[38;5;66;03m# ll should be (S, 1) or (S,)\u001b[39;00m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp.squeeze(ll)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/bayesianquilts/predictors/nn/poisson.py:66\u001b[39m, in \u001b[36mNeuralPoissonRegression.log_likelihood\u001b[39m\u001b[34m(self, data, **params)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_likelihood\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, **params):\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictive_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mlog_likelihood\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/bayesianquilts/predictors/nn/poisson.py:57\u001b[39m, in \u001b[36mNeuralPoissonRegression.predictive_distribution\u001b[39m\u001b[34m(self, data, **params)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[32m     56\u001b[39m     rv = tfd.Poisson(rate=rate)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     log_lik = \u001b[43mrv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     60\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: rate,\n\u001b[32m     61\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlog_likelihood\u001b[39m\u001b[33m\"\u001b[39m: log_lik,\n\u001b[32m     62\u001b[39m      \u001b[33m\"\u001b[39m\u001b[33mlog_rate\u001b[39m\u001b[33m\"\u001b[39m: log_rate\n\u001b[32m     63\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1287\u001b[39m, in \u001b[36mDistribution.log_prob\u001b[39m\u001b[34m(self, value, name, **kwargs)\u001b[39m\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value, name=\u001b[33m'\u001b[39m\u001b[33mlog_prob\u001b[39m\u001b[33m'\u001b[39m, **kwargs):\n\u001b[32m   1276\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Log probability density/mass function.\u001b[39;00m\n\u001b[32m   1277\u001b[39m \n\u001b[32m   1278\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1285\u001b[39m \u001b[33;03m      values of type `self.dtype`.\u001b[39;00m\n\u001b[32m   1286\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1287\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1269\u001b[39m, in \u001b[36mDistribution._call_log_prob\u001b[39m\u001b[34m(self, value, name, **kwargs)\u001b[39m\n\u001b[32m   1267\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._name_and_control_scope(name, value, kwargs):\n\u001b[32m   1268\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_log_prob\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1269\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1270\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_prob\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1271\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.math.log(\u001b[38;5;28mself\u001b[39m._prob(value, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/distributions/poisson.py:256\u001b[39m, in \u001b[36mPoisson._log_prob\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    255\u001b[39m   log_rate = \u001b[38;5;28mself\u001b[39m._log_rate_parameter_no_checks()\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m   log_probs = (\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_unnormalized_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_rate\u001b[49m\u001b[43m)\u001b[49m -\n\u001b[32m    257\u001b[39m                \u001b[38;5;28mself\u001b[39m._log_normalization(log_rate))\n\u001b[32m    258\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.force_probs_to_zero_outside_support:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Ensure the gradient wrt `rate` is zero at non-integer points.\u001b[39;00m\n\u001b[32m    260\u001b[39m     log_probs = tf.where(\n\u001b[32m    261\u001b[39m         tf.math.is_inf(log_probs),\n\u001b[32m    262\u001b[39m         dtype_util.as_numpy_dtype(log_probs.dtype)(-np.inf),\n\u001b[32m    263\u001b[39m         log_probs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/distributions/poisson.py:297\u001b[39m, in \u001b[36mPoisson._log_unnormalized_prob\u001b[39m\u001b[34m(self, x, log_rate)\u001b[39m\n\u001b[32m    294\u001b[39m safe_x = tf.maximum(\n\u001b[32m    295\u001b[39m     tf.floor(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.force_probs_to_zero_outside_support \u001b[38;5;28;01melse\u001b[39;00m x, \u001b[32m0.\u001b[39m)\n\u001b[32m    296\u001b[39m y = tf.math.multiply_no_nan(log_rate, safe_x) - tf.math.lgamma(\u001b[32m1.\u001b[39m + safe_x)\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mequal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_util\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_numpy_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43minf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/tensorflow_probability/python/internal/backend/jax/_utils.py:62\u001b[39m, in \u001b[36mcopy_docstring.<locals>.wrap\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;129m@wrapt\u001b[39m.decorator\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap\u001b[39m(wrapped, instance, args, kwargs):\n\u001b[32m     61\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m instance, wrapped\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/tensorflow_probability/python/internal/backend/jax/numpy_array.py:394\u001b[39m, in \u001b[36m_where\u001b[39m\u001b[34m(condition, x, y, name)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    393\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m np.stack(np.asarray(condition).nonzero(), axis=-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:2785\u001b[39m, in \u001b[36mwhere\u001b[39m\u001b[34m(condition, x, y, size, fill_value)\u001b[39m\n\u001b[32m   2781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2782\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEither both or neither of the x and y arguments \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2783\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33mshould be provided to jax.numpy.where, got \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2784\u001b[39m                    \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2785\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/pjit.py:261\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.no_tracing.value:\n\u001b[32m    257\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    260\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr,\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m  executable, pgle_profiler, const_args) = \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m     \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    265\u001b[39m     executable, out_tree, args_flat, out_flat, jaxpr.effects, jaxpr.consts,\n\u001b[32m    266\u001b[39m     pgle_profiler,\n\u001b[32m    267\u001b[39m     const_args)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/pjit.py:147\u001b[39m, in \u001b[36m_python_pjit_helper\u001b[39m\u001b[34m(fun, jit_info, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m   out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n\u001b[32m    145\u001b[39m       *args_flat, **p.params)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m   out_flat = \u001b[43mjit_p\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m   compiled = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m   profiler = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/core.py:633\u001b[39m, in \u001b[36mPrimitive.bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **params):\n\u001b[32m    632\u001b[39m   args = args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_canonicalization \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(canonicalize_value, args)\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_true_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/core.py:649\u001b[39m, in \u001b[36mPrimitive._true_bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    647\u001b[39m trace_ctx.set_trace(eval_trace)\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    651\u001b[39m   trace_ctx.set_trace(prev_trace)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/core.py:661\u001b[39m, in \u001b[36mPrimitive.bind_with_trace\u001b[39m\u001b[34m(self, trace, args, params)\u001b[39m\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_current_trace(trace):\n\u001b[32m    660\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_lojax(*args, **params)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    662\u001b[39m trace.process_primitive(\u001b[38;5;28mself\u001b[39m, args, params)  \u001b[38;5;66;03m# may raise lojax error\u001b[39;00m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt apply typeof to args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/interpreters/batching.py:261\u001b[39m, in \u001b[36mBatchTrace.process_primitive\u001b[39m\u001b[34m(self, p, tracers, params)\u001b[39m\n\u001b[32m    259\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m core.set_current_trace(\u001b[38;5;28mself\u001b[39m.parent_trace):\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m       val_out, dim_out = \u001b[43mfancy_primitive_batchers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m          \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvals_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m args_not_mapped:\n\u001b[32m    264\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m p.bind_with_trace(\u001b[38;5;28mself\u001b[39m.parent_trace, vals_in, params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/pjit.py:1860\u001b[39m, in \u001b[36m_pjit_batcher\u001b[39m\u001b[34m(axis_data, vals_in, dims_in, jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, donated_invars, ctx_mesh, name, keep_unused, inline, compiler_options_kvs)\u001b[39m\n\u001b[32m   1855\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mall\u001b[39m(l \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m in_layouts) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m   1856\u001b[39m         \u001b[38;5;28mall\u001b[39m(l \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m out_layouts)):\n\u001b[32m   1857\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1858\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mConcrete layouts are not supported for vmap(jit).\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1860\u001b[39m vals_out = \u001b[43mjit_p\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1861\u001b[39m \u001b[43m  \u001b[49m\u001b[43m*\u001b[49m\u001b[43mvals_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[43m  \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_jaxpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1863\u001b[39m \u001b[43m  \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1864\u001b[39m \u001b[43m  \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1865\u001b[39m \u001b[43m  \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1866\u001b[39m \u001b[43m  \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1867\u001b[39m \u001b[43m  \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1868\u001b[39m \u001b[43m  \u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1869\u001b[39m \u001b[43m  \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1870\u001b[39m \u001b[43m  \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1871\u001b[39m \u001b[43m  \u001b[49m\u001b[43minline\u001b[49m\u001b[43m=\u001b[49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1872\u001b[39m \u001b[43m  \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1874\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m vals_out, axes_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/core.py:633\u001b[39m, in \u001b[36mPrimitive.bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **params):\n\u001b[32m    632\u001b[39m   args = args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_canonicalization \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(canonicalize_value, args)\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_true_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/core.py:649\u001b[39m, in \u001b[36mPrimitive._true_bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    647\u001b[39m trace_ctx.set_trace(eval_trace)\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    651\u001b[39m   trace_ctx.set_trace(prev_trace)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/core.py:658\u001b[39m, in \u001b[36mPrimitive.bind_with_trace\u001b[39m\u001b[34m(self, trace, args, params)\u001b[39m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m: \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# try lojax error message\u001b[39;00m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_high\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43min_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m trace.requires_low:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_current_trace(trace):\n\u001b[32m    660\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_lojax(*args, **params)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/bayesianquilts/env/lib/python3.11/site-packages/jax/_src/pjit.py:1251\u001b[39m, in \u001b[36m_is_high\u001b[39m\u001b[34m(jaxpr, *_, **__)\u001b[39m\n\u001b[32m   1248\u001b[39m jit_p.multiple_results = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1249\u001b[39m jit_p.skip_canonicalization = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_is_high\u001b[39m(*_, jaxpr, **__) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m   1252\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m jaxpr.jaxpr.is_high\n\u001b[32m   1253\u001b[39m jit_p.is_high = _is_high  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Full Simulation for Table Metrics (Optimized for Memory)\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "# Initialize Likelihood and Sampler\n",
    "likelihood_fn = NeuralPoissonLikelihood(model)\n",
    "ais_sampler = AdaptiveImportanceSampler(likelihood_fn=likelihood_fn)\n",
    "\n",
    "n_simulations = 100\n",
    "n_samples = 1000\n",
    "rhos = [ 2**-r for r in range(-2, 11) ]\n",
    "\n",
    "# Split transformations\n",
    "base_transform = ['identity']\n",
    "other_transforms = ['ll', 'kl', 'var', 'mm1', 'mm2', 'pmm1', 'pmm2']\n",
    "\n",
    "print(f\"Running {n_simulations} simulations with s={n_samples} samples per run.\")\n",
    "print(f\"Rhos: {rhos}\")\n",
    "print(f\"Base Transform: {base_transform}\")\n",
    "print(f\"Other Transforms: {other_transforms}\")\n",
    "\n",
    "data_jax = {'X': jnp.array(X_data), 'y': jnp.array(y_data)}\n",
    "N_obs = data_jax['y'].shape[0]\n",
    "\n",
    "simulation_records = []\n",
    "\n",
    "for i in tqdm(range(n_simulations), desc=\"Simulations\"):\n",
    "    # 1. Sample Parameters\n",
    "    params = model.sample_mcmc(num_samples=n_samples)\n",
    "\n",
    "    # 2. Run AIS with Identity FIRST\n",
    "    results_base = ais_sampler.adaptive_is_loo(\n",
    "        data=data_jax,\n",
    "        params=params,\n",
    "        rhos=rhos,\n",
    "        variational=False, # MCMC samples\n",
    "        transformations=base_transform\n",
    "    )\n",
    "\n",
    "    # Extract identity khat (convert to numpy for mutability)\n",
    "    khat_identity = np.array(results_base['identity']['khat'])\n",
    "\n",
    "    # Identify problematic points\n",
    "    idx_bad = np.where(khat_identity >= 0.7)[0]\n",
    "\n",
    "    print(f\"Sim {i+1}: Found {len(idx_bad)} / {N_obs} data points needing adaptation.\")\n",
    "\n",
    "    # Clean up results_base immediately to free memory\n",
    "    del results_base\n",
    "    gc.collect()\n",
    "\n",
    "    # Prepare storage for this simulation's khats per method\n",
    "    # Initialize with identity khat for all (Hybrid approach: default to identity)\n",
    "    method_khats = {}\n",
    "    method_khats['identity'] = khat_identity\n",
    "    for m in other_transforms:\n",
    "        method_khats[m] = np.array(khat_identity.copy())\n",
    "\n",
    "    # 3. Process problematic points with other methods\n",
    "    if len(idx_bad) > 0:\n",
    "        batch_size = 16\n",
    "        num_batches = int(np.ceil(len(idx_bad) / batch_size))\n",
    "\n",
    "        for b in range(num_batches):\n",
    "            batch_idx = idx_bad[b*batch_size : (b+1)*batch_size]\n",
    "\n",
    "            # Create data subset for BAD points in this batch\n",
    "            # JAX arrays can be indexed by numpy arrays\n",
    "            data_subset = {\n",
    "                'X': data_jax['X'][batch_idx],\n",
    "                'y': data_jax['y'][batch_idx]\n",
    "            }\n",
    "\n",
    "            # Run AIS on subset\n",
    "            results_subset = ais_sampler.adaptive_is_loo(\n",
    "                data=data_subset,\n",
    "                params=params,\n",
    "                rhos=rhos,\n",
    "                variational=False,\n",
    "                transformations=other_transforms\n",
    "            )\n",
    "\n",
    "            # Update method stats\n",
    "            for base_method in other_transforms:\n",
    "                # Find best khat for this base method (across rhos)\n",
    "                khat_arrays = []\n",
    "                for key, res in results_subset.items():\n",
    "                    if key == 'best': continue\n",
    "                    if key == base_method or key.startswith(base_method + '_'):\n",
    "                        khat_arrays.append(res['khat'])\n",
    "\n",
    "                if khat_arrays:\n",
    "                    # Min over rhos for this method on the SUBSET\n",
    "                    min_khat_subset = np.array(np.min(np.stack(khat_arrays), axis=0))\n",
    "\n",
    "                    # Update the main array at the bad indices for this batch\n",
    "                    method_khats[base_method][batch_idx] = min_khat_subset\n",
    "\n",
    "            # Clean up batch results\n",
    "            del results_subset\n",
    "            gc.collect()\n",
    "\n",
    "    # Define Groups\n",
    "    groups = {\n",
    "        'Base': ['identity'],\n",
    "        'PMM1': ['pmm1'],\n",
    "        'PMM2': ['pmm2'],\n",
    "        'KL': ['kl'],\n",
    "        'Var': ['var'],\n",
    "        'Ours_Combined': ['pmm1', 'pmm2', 'kl', 'var'],\n",
    "        'LL': ['ll'],\n",
    "        'MM1': ['mm1'],\n",
    "        'MM2': ['mm2'],\n",
    "        'Full': other_transforms + base_transform\n",
    "    }\n",
    "\n",
    "    sim_counts = {}\n",
    "    for group_name, methods in groups.items():\n",
    "        grouped_khats = []\n",
    "        for m in methods:\n",
    "             if m in method_khats:\n",
    "                 grouped_khats.append(method_khats[m])\n",
    "\n",
    "        if grouped_khats:\n",
    "            # Best khat across ANY method in the group for each obs\n",
    "            # Take minimum khat (best adaptation)\n",
    "            best_group_khat = np.min(np.stack(grouped_khats), axis=0)\n",
    "            # Count FAILURES (khat > 0.7)\n",
    "            n_failures = np.sum(best_group_khat > 0.7)\n",
    "            sim_counts[group_name] = n_failures\n",
    "        else:\n",
    "            sim_counts[group_name] = np.nan\n",
    "\n",
    "    simulation_records.append(sim_counts)\n",
    "\n",
    "# 4. Aggregate Statistics across simulations\n",
    "df_sims = pd.DataFrame(simulation_records)\n",
    "stats = df_sims.agg(['mean', 'std'])\n",
    "\n",
    "print(\"\\n--- Table Metrics: Unsuccessful Adaptations (Roaches/PR) ---\")\n",
    "print(stats.round(1))\n",
    "\n",
    "# Optional: Format for LaTeX\n",
    "print(\"\\nLaTeX Format (Mean \\u005cpm Std):\")\n",
    "for col in df_sims.columns:\n",
    "    m = stats.loc['mean', col]\n",
    "    s = stats.loc['std', col]\n",
    "    print(f\"{col}: {m:.1f} \\u005cpm {s:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a24317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}