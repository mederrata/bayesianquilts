{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from skimpy import skim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 14:33:53.541543: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-08 14:33:53.579252: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-08 14:33:53.579286: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-08 14:33:53.580707: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-08 14:33:53.587327: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-08 14:33:53.587760: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-08 14:33:54.530930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from bayesianquilts.models.logistic_regression import LogisticRegression\n",
    "from bayesianquilts.models.logistic_regression_reparam import LogisticRegression2\n",
    "\n",
    "from bayesianquilts.metrics.classification import classification_metrics\n",
    "from bayesianquilts.sampler import psis, nppsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0 0.23.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__, tfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "\n",
    "Using the example from here: https://www.tensorflow.org/guide/core/logistic_regression_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128000, 21)\n"
     ]
    }
   ],
   "source": [
    "use_saved = False\n",
    "\n",
    "cdc_diabetes_health_indicators = fetch_ucirepo(id=891) \n",
    "\n",
    "\n",
    "# data (as pandas dataframes) \n",
    "batch_size=128\n",
    "X_ = cdc_diabetes_health_indicators.data.features[:batch_size*1000]\n",
    "y_ = cdc_diabetes_health_indicators.data.targets[:batch_size*1000]\n",
    "\n",
    "X_ = X_.fillna(0)\n",
    "\n",
    "X_scaled = (X_ - X_.mean())/X_.std()\n",
    "X_scaled = X_scaled.fillna(0)\n",
    "n = X_scaled.shape[0]\n",
    "p = X_scaled.shape[1]\n",
    "\n",
    "print((n, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke',\n",
       "       'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies',\n",
       "       'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth',\n",
       "       'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education',\n",
       "       'Income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128000, 21)\n"
     ]
    }
   ],
   "source": [
    "X_scaled = (X_scaled - X_scaled.mean())/X_scaled.std()\n",
    "X_scaled = X_scaled.fillna(0)\n",
    "n = X_scaled.shape[0]\n",
    "p = X_scaled.shape[1]\n",
    "\n",
    "print((n, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tfdata = tf.data.Dataset.from_tensor_slices({'X': X_scaled, 'y':y_})\n",
    "\n",
    "def data_factory_factory(batch_size=batch_size, repeat=False, shuffle=False):\n",
    "    tfdata_ = tfdata\n",
    "    def data_factory(batch_size=batch_size):\n",
    "        if shuffle:\n",
    "            out = tfdata_.shuffle(batch_size*10)\n",
    "        else:\n",
    "            out = tfdata_\n",
    "        \n",
    "        if repeat:\n",
    "            out = out.repeat()\n",
    "        return out.batch(batch_size)\n",
    "    return data_factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_saved:\n",
    "    lr_model = LogisticRegression2(dim_regressors=p)\n",
    "        \n",
    "else:\n",
    "    import dill as pickle\n",
    "    import gzip\n",
    "    with gzip.open(\"breast.gz\", 'rb') as f:\n",
    "        lr_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Bayesian minibatch ADVI inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float64, numpy=array([-186.60597405, -180.69228637])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = next(iter(data_factory_factory()()))\n",
    "lr_model.unormalized_log_prob(test, **lr_model.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimization for 6000 steps of 20 accumulated batches, checking every 1000 steps\n",
      "Saved a checkpoint: /tmp/tfcheckpoints/9c4167d7-3e10-48ff-b558-8cf78f00a267/9c4167d7-3e10-48ff-b558-8cf78f00a267-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 998/6000 [01:19<06:33, 12.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: average-batch loss:136.6769204543936 rel loss: 7.31652422863654e+305\n",
      "Saved a checkpoint: /tmp/tfcheckpoints/9c4167d7-3e10-48ff-b558-8cf78f00a267/9c4167d7-3e10-48ff-b558-8cf78f00a267-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1998/6000 [02:38<05:11, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000: average-batch loss:116.70802460321639 rel loss: 0.17110130960632244\n",
      "Saved a checkpoint: /tmp/tfcheckpoints/9c4167d7-3e10-48ff-b558-8cf78f00a267/9c4167d7-3e10-48ff-b558-8cf78f00a267-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 2998/6000 [03:56<03:52, 12.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000: average-batch loss:103.70636365953628 rel loss: 0.12536994341411897\n",
      "Saved a checkpoint: /tmp/tfcheckpoints/9c4167d7-3e10-48ff-b558-8cf78f00a267/9c4167d7-3e10-48ff-b558-8cf78f00a267-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 3998/6000 [05:13<02:34, 12.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000: average-batch loss:98.67166634596815 rel loss: 0.05102475208957343\n",
      "Saved a checkpoint: /tmp/tfcheckpoints/9c4167d7-3e10-48ff-b558-8cf78f00a267/9c4167d7-3e10-48ff-b558-8cf78f00a267-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 4998/6000 [06:31<01:18, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5000: average-batch loss:97.49261044684178 rel loss: 0.012093797609094207\n",
      "Saved a checkpoint: /tmp/tfcheckpoints/9c4167d7-3e10-48ff-b558-8cf78f00a267/9c4167d7-3e10-48ff-b558-8cf78f00a267-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5998/6000 [07:52<00:00, 12.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6000: average-batch loss:96.37442834705321 rel loss: 0.01160247711936503\n",
      "Saved a checkpoint: /tmp/tfcheckpoints/9c4167d7-3e10-48ff-b558-8cf78f00a267/9c4167d7-3e10-48ff-b558-8cf78f00a267-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [07:52<00:00, 12.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminating because we are out of iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [07:52<00:00, 12.70it/s]\n"
     ]
    }
   ],
   "source": [
    "if not use_saved:\n",
    "\n",
    "    losses = lr_model.fit(\n",
    "        data_factory_factory(shuffle=True, repeat=True),\n",
    "        dataset_size=n,\n",
    "        batches_per_step=20,\n",
    "        check_every=int(n/batch_size),\n",
    "        batch_size=batch_size,\n",
    "        num_steps=6000,\n",
    "        max_decay_steps=20,\n",
    "        max_plateau_epochs=20,\n",
    "        sample_size=32,\n",
    "        learning_rate=0.0005)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [01:45,  9.49it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1000 has 0 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     pred \u001b[38;5;241m=\u001b[39m lr_model\u001b[38;5;241m.\u001b[39mpredictive_distribution(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreduce_mean(pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m bench \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_factory_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutcome_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mby_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     14\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(bench[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauroc\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfpr\u001b[39m\u001b[38;5;124m'\u001b[39m], bench[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauroc\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtpr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/workspace/bayesianquilts/bayesianquilts/metrics/classification.py:102\u001b[0m, in \u001b[0;36mclassification_metrics\u001b[0;34m(data_factory, prediction_fn, preprocessing_fn, by_vars, outcome_label, method, save_file)\u001b[0m\n\u001b[1;32m    100\u001b[0m probs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(probs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m collected_data\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 102\u001b[0m     collected_data[k] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollected_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    104\u001b[0m computed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: probs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcollected_data})\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_file:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1000 has 0 dimension(s)"
     ]
    }
   ],
   "source": [
    "def prediction_fn(data):\n",
    "    params = lr_model.sample(128)\n",
    "    pred = lr_model.predictive_distribution(data, **params)[\"logits\"]\n",
    "    return tf.reduce_mean(pred, axis=0)\n",
    "\n",
    "bench = classification_metrics(\n",
    "    data_factory=data_factory_factory(),\n",
    "    prediction_fn=prediction_fn,\n",
    "    outcome_label='y',\n",
    "    by_vars=[]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 2))\n",
    "ax[0].plot(bench['auroc']['fpr'], bench['auroc']['tpr'])\n",
    "ax[0].text(0.5, 0.1, f\"AUROC: {round(bench['auroc']['auroc'], 2)}\")\n",
    "ax[0].set_xlim((0, 1))\n",
    "ax[0].set_ylim((0, 1))\n",
    "ax[0].set_title(\"ROC\")\n",
    "\n",
    "ax[1].plot(bench['auprc']['recall'], bench['auprc']['precision'])\n",
    "ax[1].text(0.5, 0.8, f\"AUPRC: {round(bench['auprc']['auprc'], 2)}\")\n",
    "ax[1].set_title(\"Precision-Recall\")\n",
    "ax[1].set_xlim((0, 1))\n",
    "ax[1].set_ylim((0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(iter(data_factory_factory()()))\n",
    "param_test = lr_model.sample(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = tf.reduce_mean(param_test['beta__'], axis=0)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 1 ))\n",
    "im = ax.pcolormesh(beta, cmap='seismic_r')\n",
    "_ = ax.set_xticks(0.5 + np.arange(len(X_.columns)), X_.columns, rotation=90)\n",
    "plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 396\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    352\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_loo\u001b[39m\u001b[38;5;124m\"\u001b[39m: p_loo_I,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         },\n\u001b[1;32m    391\u001b[0m     }\n\u001b[1;32m    394\u001b[0m param_test \u001b[38;5;241m=\u001b[39m lr_model\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m--> 396\u001b[0m loo \u001b[38;5;241m=\u001b[39m adaptive_is_loo(lr_model, \u001b[43mtest_batch\u001b[49m, param_test, \u001b[38;5;241m1.\u001b[39m, variational\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# loo = adaptive_is_loo(lr_model, test_batch, param_test, 0.01, variational=False)\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m T \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVar\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_batch' is not defined"
     ]
    }
   ],
   "source": [
    "def entropy(probs):\n",
    "    return -tf.math.xlogy(probs, probs)\n",
    "\n",
    "\n",
    "def adaptive_is_loo(self, data, params, hbar=1.0, variational=True):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        data (_type_): _description_\n",
    "        params (_type_): _description_\n",
    "        hbar (float, optional): _description_. Defaults to 1.0.\n",
    "        variational (bool, optional):\n",
    "            Should we trust the variational approximation?\n",
    "            If False, assumes that one is passing in all the data at once in a single batch.\n",
    "            Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # scaled (theta - bar(theta))/Sigma\n",
    "    beta = params[\"beta__\"]\n",
    "    intercept = params[\"intercept__\"]\n",
    "    X = tf.cast(data[\"X\"], self.dtype)\n",
    "    y = tf.cast(data[\"y\"], self.dtype)[:, 0]\n",
    "    mu = tf.reduce_sum(beta * X, axis=-1) + intercept[..., 0]\n",
    "    sigma = tf.math.sigmoid(mu)\n",
    "    ell = y * (sigma) + (1 - y) * (1 - sigma)\n",
    "    log_ell = tf.math.xlogy(y, sigma) + tf.math.xlogy(1 - y, 1 - sigma)\n",
    "    log_ell_prime = y * (1 - sigma) - (1 - y) * sigma\n",
    "    log_ell_doubleprime = -sigma * (1 - sigma)\n",
    "\n",
    "    \"\"\"\n",
    "    sigma.shape is samples x datapoints\n",
    "    \"\"\"\n",
    "\n",
    "    # compute # \\nabla\\log\\pi(\\btheta|\\calD)\n",
    "    if variational:\n",
    "        # \\nabla\\log\\pi = -\\Sigma^{-1}(theta - \\bar{\\theta})\n",
    "        grad_log_pi = tf.concat(\n",
    "            [\n",
    "                -(intercept - self.surrogate_distribution.model[\"intercept__\"].mean())\n",
    "                / self.surrogate_distribution.model[\"intercept__\"].variance(),\n",
    "                -(beta - self.surrogate_distribution.model[\"beta__\"].mean())\n",
    "                / self.surrogate_distribution.model[\"beta__\"].variance(),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        intercept_sd = (\n",
    "            self.surrogate_distribution.model[\"intercept__\"].variance() ** 0.5\n",
    "        )\n",
    "        beta_sd = self.surrogate_distribution.model[\"beta__\"].variance() ** 0.5\n",
    "\n",
    "        log_pi = self.surrogate_distribution.model[\"beta__\"].log_prob(\n",
    "            params[\"beta__\"]\n",
    "        ) + self.surrogate_distribution.model[\"intercept__\"].log_prob(\n",
    "            params[\"intercept__\"]\n",
    "        )\n",
    "        log_pi -= tf.reduce_max(log_pi, axis=0)\n",
    "        # log_pi.shape: [samples]\n",
    "    else:\n",
    "        \"\"\"\n",
    "        Recall Bayes rule:\n",
    "        \\log pi(\\btheta|\\calD) = \\sum_i\\log ell_i(\\btheta) + \\log\\pi(\\btheta) + const\n",
    "\n",
    "        so\n",
    "        \\nabla\\log\\pi(\\btheta|\\calD) = \\sum_i (ell_i)'x + grad\\log\\pi(\\btheta)\n",
    "\n",
    "        \"\"\"\n",
    "        log_pi = tf.reduce_sum(log_ell, axis=1, keepdims=True)[:, 0]\n",
    "        log_pi += self.prior_distribution.log_prob(\n",
    "            {\n",
    "                \"regression_model\": {\n",
    "                    k: tf.cast(params[k], self.dtype) for k in self.regression_var_list\n",
    "                },\n",
    "                \"intercept_model\": {\n",
    "                    k: tf.cast(params[k], self.dtype) for k in self.intercept_var_list\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "        # pi \\propto\n",
    "        grad_log_pi = tf.reduce_sum(\n",
    "            log_ell[..., tf.newaxis] * X, axis=1, keepdims=True\n",
    "        )  # TODO NEED PRIOR TERM\n",
    "\n",
    "        prior_intercept_sd = (\n",
    "            self.prior_distribution.model[\"intercept_model\"]\n",
    "            .model[\"intercept__\"]\n",
    "            .variance()\n",
    "            ** 0.5\n",
    "        )\n",
    "        prior_beta_sd = params[\"global_scale\"]\n",
    "\n",
    "        intercept_sd = tf.math.reduce_std(intercept, 0, keepdims=True)\n",
    "        beta_sd = tf.math.reduce_std(beta, 0, keepdims=True)\n",
    "\n",
    "    # log-likelihood descent\n",
    "\n",
    "    def T_ll():\n",
    "        Q_beta = -log_ell_prime[..., tf.newaxis] * X\n",
    "        Q_intercept = -log_ell_prime[..., tf.newaxis]\n",
    "\n",
    "        standardized = tf.concat(\n",
    "            [Q_beta / beta_sd, Q_intercept / intercept_sd], axis=-1\n",
    "        )\n",
    "        standardized = tf.reduce_max(tf.math.abs(standardized), axis=-1)\n",
    "        standardized = tf.reduce_max(standardized, axis=0, keepdims=True)[\n",
    "            ..., tf.newaxis\n",
    "        ]\n",
    "\n",
    "        h = hbar / standardized\n",
    "        logJ = tf.math.log1p(\n",
    "            tf.math.abs(\n",
    "                h\n",
    "                * (1 + tf.math.reduce_sum(X**2, -1, keepdims=True))[tf.newaxis, :, :]\n",
    "                * (sigma * (1 - sigma))[..., tf.newaxis]\n",
    "            )[..., 0]\n",
    "        )\n",
    "        beta_ll = beta + h * Q_beta\n",
    "        intercept_ll = intercept + h * Q_intercept\n",
    "        return beta_ll, intercept_ll, logJ\n",
    "\n",
    "    def T_kl():\n",
    "\n",
    "        Q_beta = ((-1) ** y * tf.math.exp(log_pi[..., tf.newaxis] + mu * (1 - 2 * y)))[\n",
    "            ..., tf.newaxis\n",
    "        ] * data[\"X\"]\n",
    "        Q_intercept = (\n",
    "            (-1) ** y * tf.math.exp(log_pi[..., tf.newaxis] + mu * (1 - 2 * y))\n",
    "        )[..., tf.newaxis]\n",
    "\n",
    "        dQ = (\n",
    "            (-1) ** y[tf.newaxis, :]\n",
    "            * tf.math.exp(log_pi[..., tf.newaxis] + mu * (1 - 2 * y[tf.newaxis, :]))\n",
    "            * (\n",
    "                grad_log_pi[..., 0]\n",
    "                + (1 - 2 * y)[tf.newaxis, :]\n",
    "                + tf.reduce_sum(\n",
    "                    X * (grad_log_pi[..., 1:] + (1 - 2 * y)[:, tf.newaxis] * X), axis=-1\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        standardized = tf.concat(\n",
    "            [Q_beta / beta_sd, Q_intercept / intercept_sd], axis=-1\n",
    "        )\n",
    "        standardized = tf.reduce_max(tf.math.abs(standardized), axis=-1)\n",
    "        standardized = tf.reduce_max(standardized, axis=0, keepdims=True)[\n",
    "            ..., tf.newaxis\n",
    "        ]\n",
    "\n",
    "        h = hbar / standardized\n",
    "\n",
    "        intercept_kl = intercept + h * Q_intercept\n",
    "        beta_kl = beta + h * Q_beta\n",
    "\n",
    "        logJ = tf.math.log1p(tf.math.abs(h[..., 0] * dQ))\n",
    "        return beta_kl, intercept_kl, logJ\n",
    "\n",
    "    # variance descent -(log ell)'/l\n",
    "\n",
    "    def T_I():\n",
    "        Q = tf.zeros_like(log_ell)\n",
    "        return (\n",
    "            beta + Q[..., tf.newaxis],\n",
    "            intercept + Q[..., tf.newaxis],\n",
    "            tf.zeros_like(Q),\n",
    "        )\n",
    "\n",
    "    def T_var():\n",
    "\n",
    "        Q_beta = (\n",
    "            (-1) ** y * tf.math.exp(log_pi[..., tf.newaxis] + 2 * mu * (1 - 2 * y))\n",
    "        )[..., tf.newaxis] * data[\"X\"]\n",
    "        Q_intercept = (\n",
    "            (-1) ** y * tf.math.exp(log_pi[..., tf.newaxis] + 2 * mu * (1 - 2 * y))\n",
    "        )[..., tf.newaxis]\n",
    "\n",
    "        dQ = (\n",
    "            (-1) ** y[tf.newaxis, :]\n",
    "            * tf.math.exp(log_pi[..., tf.newaxis] + 2 * mu * (1 - 2 * y[tf.newaxis, :]))\n",
    "            * (\n",
    "                grad_log_pi[..., 0]\n",
    "                + (1 - 2 * y)[tf.newaxis, :]\n",
    "                + tf.reduce_sum(\n",
    "                    X * (grad_log_pi[..., 1:] + 2 * (1 - 2 * y)[:, tf.newaxis] * X),\n",
    "                    axis=-1,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        standardized = tf.concat(\n",
    "            [Q_beta / beta_sd, Q_intercept / intercept_sd], axis=-1\n",
    "        )\n",
    "        standardized = tf.reduce_max(tf.math.abs(standardized), axis=-1)\n",
    "        standardized = tf.reduce_max(standardized, axis=0, keepdims=True)[\n",
    "            ..., tf.newaxis\n",
    "        ]\n",
    "\n",
    "        h = hbar / standardized\n",
    "\n",
    "        intercept_kl = intercept + h * Q_intercept\n",
    "        beta_kl = beta + h * Q_beta\n",
    "\n",
    "        logJ = tf.math.log1p(tf.math.abs(h[..., 0] * dQ))\n",
    "        return beta_kl, intercept_kl, logJ\n",
    "\n",
    "    def IS(Q):\n",
    "        beta_new, intercept_new, logJ = Q()\n",
    "        mu_new = tf.reduce_sum(beta_new * X, axis=-1) + intercept_new[..., 0]\n",
    "        sigma_new = tf.math.sigmoid(mu_new)\n",
    "        ell_new = y * (sigma_new) + (1 - y) * (1 - sigma_new)\n",
    "        log_ell_new = tf.math.xlogy(y, sigma_new) + tf.math.xlogy(1 - y, 1 - sigma_new)\n",
    "        transformed = params.copy()\n",
    "        transformed[\"beta__\"] = beta_new[..., tf.newaxis, :]\n",
    "        transformed[\"intercept__\"] = intercept_new[..., tf.newaxis, :]\n",
    "        transformed[\"global_scale\"] = transformed[\"global_scale\"][..., tf.newaxis, :]\n",
    "        transformed[\"global_scale_aux\"] = transformed[\"global_scale_aux\"][\n",
    "            ..., tf.newaxis, :\n",
    "        ]\n",
    "\n",
    "        if variational:\n",
    "            # We trust the variational approximation, so \\hat{pi} = pi\n",
    "            # N_samples x N_data\n",
    "            delta_log_pi = (\n",
    "                self.surrogate_distribution.log_prob(transformed)\n",
    "                - log_pi[:, tf.newaxis]\n",
    "            )\n",
    "            delta_log_pi = delta_log_pi - tf.reduce_max(\n",
    "                delta_log_pi, axis=0, keepdims=True\n",
    "            )\n",
    "            pass\n",
    "        else:\n",
    "            # we don't trust the variational approximation\n",
    "            # Need to compute log_pi directly by summing over the likelihood\n",
    "\n",
    "            ell_cross = (\n",
    "                tf.reduce_sum(beta_new[..., tf.newaxis, :] * data[\"X\"], -1)\n",
    "                + intercept_new\n",
    "            )\n",
    "            ell_cross = tf.reduce_sum(\n",
    "                ell_cross, -1\n",
    "            )  # this is the likelihood portion of pi(T(theta))\n",
    "\n",
    "            log_pi_new = self.prior_distribution.log_prob(\n",
    "                {\n",
    "                    \"regression_model\": {\n",
    "                        k: tf.cast(transformed[k], self.dtype)\n",
    "                        for k in self.regression_var_list\n",
    "                    },\n",
    "                    \"intercept_model\": {\n",
    "                        k: tf.cast(transformed[k], self.dtype)\n",
    "                        for k in self.intercept_var_list\n",
    "                    },\n",
    "                }\n",
    "            )[:, tf.newaxis]\n",
    "            log_pi_new += ell_cross\n",
    "            log_pi_old = self.surrogate_distribution.log_prob(params)\n",
    "            # Incorporate the prior\n",
    "            delta_log_pi = log_pi_new - log_pi_old[:, tf.newaxis]\n",
    "            delta_log_pi = delta_log_pi - tf.reduce_max(\n",
    "                delta_log_pi, axis=0, keepdims=True\n",
    "            )\n",
    "\n",
    "        log_eta_weights = delta_log_pi - log_ell_new + logJ\n",
    "        psis_weights, khat = nppsis.psislw(log_eta_weights)\n",
    "        \n",
    "        eta_weights = tf.math.exp(log_eta_weights)\n",
    "        eta_weights = eta_weights / tf.reduce_sum(eta_weights, axis=0, keepdims=True)\n",
    "\n",
    "        psis_weights = tf.math.exp(psis_weights)\n",
    "        psis_weights = psis_weights / tf.math.reduce_sum(\n",
    "            psis_weights, axis=0, keepdims=True\n",
    "        )\n",
    "\n",
    "        weight_entropy = self.entropy(eta_weights)\n",
    "        psis_entropy = self.entropy(psis_weights)\n",
    "\n",
    "        p_loo_new = tf.reduce_sum(sigma_new * eta_weights, axis=0)\n",
    "        p_loo_psis = tf.reduce_sum(sigma_new * psis_weights, axis=0)\n",
    "        p_loo_sd = tf.math.reduce_std(sigma_new * eta_weights, axis=0)\n",
    "        ll_loo_new = tf.reduce_sum(eta_weights * ell_new, axis=0)\n",
    "        ll_loo_psis = tf.reduce_sum(psis_weights * ell_new, axis=0)\n",
    "        ll_loo_sd = tf.math.reduce_std(eta_weights * ell_new, axis=0)\n",
    "        return (\n",
    "            eta_weights,\n",
    "            psis_weights,\n",
    "            p_loo_new,\n",
    "            p_loo_sd,\n",
    "            ll_loo_new,\n",
    "            ll_loo_sd,\n",
    "            weight_entropy,\n",
    "            khat,\n",
    "            p_loo_psis,\n",
    "            ll_loo_psis,\n",
    "        )\n",
    "    (\n",
    "        eta_I,\n",
    "        eta_I_psis,\n",
    "        p_loo_I,\n",
    "        p_loo_I_sd,\n",
    "        ll_loo_I,\n",
    "        ll_loo_I_sd,\n",
    "        S_I,\n",
    "        k_I,\n",
    "        p_psis_I,\n",
    "        ll_psis_I,\n",
    "    ) = IS(T_I)\n",
    "    (\n",
    "        eta_ll,\n",
    "        eta_ll_psis,\n",
    "        p_loo_ll,\n",
    "        p_loo_ll_sd,\n",
    "        ll_loo_ll,\n",
    "        ll_loo_ll_sd,\n",
    "        S_ll,\n",
    "        k_ll,\n",
    "        p_psis_ll,\n",
    "        ll_psis_ll,\n",
    "    ) = IS(T_ll)\n",
    "\n",
    "    (\n",
    "        eta_kl,\n",
    "        eta_kl_psis,\n",
    "        p_loo_kl,\n",
    "        p_loo_kl_sd,\n",
    "        ll_loo_kl,\n",
    "        ll_loo_kl_sd,\n",
    "        S_kl,\n",
    "        k_kl,\n",
    "        p_psis_kl,\n",
    "        ll_psis_kl,\n",
    "    ) = IS(T_kl)\n",
    "\n",
    "    (\n",
    "        eta_var,\n",
    "        eta_var_psis,\n",
    "        p_loo_var,\n",
    "        p_loo_var_sd,\n",
    "        ll_loo_var,\n",
    "        ll_loo_var_sd,\n",
    "        S_var,\n",
    "        k_var,\n",
    "        p_psis_var,\n",
    "        ll_psis_var,\n",
    "    ) = IS(T_var)\n",
    "\n",
    "    # kl descent\n",
    "\n",
    "    return {\n",
    "        \"I\": {\n",
    "            \"p_loo\": p_loo_I,\n",
    "            \"p_loo_sd\": p_loo_I_sd,\n",
    "            \"ll_loo\": ll_loo_I,\n",
    "            \"ll_loo_sd\": ll_loo_I_sd,\n",
    "            \"S\": S_I,\n",
    "            \"khat\": k_I,\n",
    "            \"p_psis\": p_psis_I,\n",
    "            \"ll_psis\": ll_psis_I,\n",
    "        },\n",
    "        \"KL\": {\n",
    "            \"p_loo\": p_loo_kl,\n",
    "            \"p_loo_sd\": p_loo_kl_sd,\n",
    "            \"ll_loo\": ll_loo_kl,\n",
    "            \"ll_loo_sd\": ll_loo_kl_sd,\n",
    "            \"S\": S_kl,\n",
    "            \"khat\": k_kl,\n",
    "            \"p_psis\": p_psis_kl,\n",
    "            \"ll_psis\": ll_psis_kl,\n",
    "        },\n",
    "        \"LL\": {\n",
    "            \"p_loo\": p_loo_kl,\n",
    "            \"p_loo_sd\": p_loo_kl_sd,\n",
    "            \"ll_loo\": ll_loo_kl,\n",
    "            \"ll_loo_sd\": ll_loo_kl_sd,\n",
    "            \"S\": S_ll,\n",
    "            \"khat\": k_ll,\n",
    "            \"p_psis\": p_psis_ll,\n",
    "            \"ll_psis\": ll_psis_ll,\n",
    "        },\n",
    "        \"Var\": {\n",
    "            \"p_loo\": p_loo_var,\n",
    "            \"p_loo_sd\": p_loo_var_sd,\n",
    "            \"ll_loo\": ll_loo_var,\n",
    "            \"ll_loo_sd\": ll_loo_var_sd,\n",
    "            \"S\": S_var,\n",
    "            \"khat\": k_var,\n",
    "            \"p_psis\": p_psis_var,\n",
    "            \"ll_psis\": ll_psis_var,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "param_test = lr_model.sample(256)\n",
    "\n",
    "loo = adaptive_is_loo(lr_model, test_batch, param_test, 1., variational=True)\n",
    "\n",
    "# loo = adaptive_is_loo(lr_model, test_batch, param_test, 0.01, variational=False)\n",
    "\n",
    "\n",
    "for T in [\"I\", \"LL\", \"KL\", \"Var\"]:\n",
    "    print(\n",
    "        f\"{T}: {np.sqrt(np.sum(loo[T]['p_loo_sd']**2))} entropy: {np.sqrt(np.sum(loo[T]['S']))} khat>0.5: {np.sum(loo[T]['khat']>0.5)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo['I']['khat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute LOO AU ROC/PRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.2\n",
    "def prediction_fn_stepaway(data):\n",
    "    params = lr_model.sample(100)\n",
    "    loo = adaptive_is_loo(lr_model, data, params, step_size)\n",
    "    return loo[\"KL\"]['p_psis']\n",
    "\n",
    "def prediction_fn_loo(data):\n",
    "    params = lr_model.sample(100)\n",
    "    pred = adaptive_is_loo(lr_model, data, params, step_size)[\"I\"]['p_psis']\n",
    "    return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_stepaway_loo = classification_metrics(\n",
    "    data_factory=data_factory_factory(batch_size=2000),\n",
    "    prediction_fn=prediction_fn_stepaway,\n",
    "    outcome_label='y',\n",
    "    by_vars=[]\n",
    ")\n",
    "\n",
    "bench_loo = classification_metrics(\n",
    "    data_factory=data_factory_factory(batch_size=2000),\n",
    "    prediction_fn=prediction_fn_loo,\n",
    "    outcome_label='y',\n",
    "    by_vars=[]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-away LOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 2))\n",
    "ax[0].plot(bench_stepaway_loo['auroc']['fpr'], bench_stepaway_loo['auroc']['tpr'])\n",
    "ax[0].text(0.5, 0.1, f\"AUROC: {round(bench_stepaway_loo['auroc']['auroc'], 2)}\")\n",
    "ax[0].set_xlim((0, 1))\n",
    "ax[0].set_ylim((0, 1))\n",
    "ax[0].set_title(\"ROC\")\n",
    "\n",
    "ax[1].plot(bench_stepaway_loo['auprc']['recall'], bench_stepaway_loo['auprc']['precision'])\n",
    "ax[1].text(0.5, 0.8, f\"AUPRC: {round(bench_stepaway_loo['auprc']['auprc'], 2)}\")\n",
    "ax[1].set_title(\"Precision-Recall\")\n",
    "ax[1].set_xlim((0, 1))\n",
    "ax[1].set_ylim((0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOO w/o stepaway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 2))\n",
    "ax[0].plot(bench_loo['auroc']['fpr'], bench_loo['auroc']['tpr'])\n",
    "ax[0].text(0.5, 0.1, f\"AUROC: {round(bench_loo['auroc']['auroc'], 2)}\")\n",
    "ax[0].set_xlim((0, 1))\n",
    "ax[0].set_ylim((0, 1))\n",
    "ax[0].set_title(\"ROC\")\n",
    "\n",
    "ax[1].plot(bench_loo['auprc']['recall'], bench_loo['auprc']['precision'])\n",
    "ax[1].text(0.5, 0.8, f\"AUPRC: {round(bench_loo['auprc']['auprc'], 2)}\")\n",
    "ax[1].set_title(\"Precision-Recall\")\n",
    "ax[1].set_xlim((0, 1))\n",
    "ax[1].set_ylim((0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench['auroc']['auroc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = lr_model.sample(200)\n",
    "loo_out = []\n",
    "for batch in tqdm(iter(data_factory_factory(batch_size=1000, repeat=False)())):\n",
    "    loo_out += [adaptive_is_loo(lr_model, batch, params, 0.5, variational=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo_out[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "khat = {}\n",
    "p_loo = {}\n",
    "for method in [\"I\", \"LL\", \"KL\", \"Var\"]:\n",
    "    khat[method] = np.concatenate([out[method][\"khat\"] for out in loo_out], axis=0)\n",
    "    p_loo[method] = np.concatenate([out[method][\"p_loo\"] for out in loo_out], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "khat = pd.DataFrame(khat)\n",
    "_ = pd.plotting.scatter_matrix(khat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_loo = pd.DataFrame(p_loo)\n",
    "_ = pd.plotting.scatter_matrix(p_loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "khat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "khat[khat.I > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(khat>0.5).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
