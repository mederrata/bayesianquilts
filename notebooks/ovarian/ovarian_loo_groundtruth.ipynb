{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Patched _JointDistributionNamedSpec with _structure_with_callables=None\n",
                        "TFP Specs patched successfully.\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "from jax.scipy import special\n",
                "from tqdm import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "import importlib.resources\n",
                "import warnings\n",
                "from typing import Any, Dict\n",
                "from tensorflow_probability.substrates import jax as tfp\n",
                "tfd = tfp.distributions\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "jax.config.update(\"jax_enable_x64\", True)\n",
                "# jax.config.update(\"jax_disable_jit\", True) # Only if needed\n",
                "\n",
                "from bayesianquilts.metrics.ais import AdaptiveImportanceSampler, LikelihoodFunction\n",
                "\n",
                "data_dir = os.path.expanduser(\"~/Library/CloudStorage/Box-Box/bouldering/ovarian\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# -----------------------------------------------------\n",
                "# 1. Define Reparameterized Likelihood Class\n",
                "# -----------------------------------------------------\n",
                "\n",
                "class OvarianReparametrizedLikelihood(LikelihoodFunction):\n",
                "    def __init__(self, data_dict, priors):\n",
                "        # Store constants only. Data (X, y) comes at calls.\n",
                "        self.constants = {k: jnp.asarray(v, dtype=jnp.float64) for k, v in data_dict['constants'].items()}\n",
                "        # Keep reference to FULL data for reconstruction or if needed, but not primarily for LL\n",
                "        self.X_full = jnp.asarray(data_dict['X'], dtype=jnp.float64)\n",
                "        self.y_full = jnp.asarray(data_dict['y'], dtype=jnp.float64)\n",
                "\n",
                "    def _get_params(self, params):\n",
                "        beta0 = jnp.asarray(params['beta0'], dtype=jnp.float64)\n",
                "        z = jnp.asarray(params['z'], dtype=jnp.float64)\n",
                "        tau = jnp.exp(jnp.asarray(params['log_tau'], dtype=jnp.float64))\n",
                "        lam = jnp.exp(jnp.asarray(params['log_lambda'], dtype=jnp.float64))\n",
                "        caux = jnp.exp(jnp.asarray(params['log_caux'], dtype=jnp.float64))\n",
                "        return beta0, z, tau, lam, caux\n",
                "\n",
                "    def _compute_beta(self, z, tau, lam, caux):\n",
                "        c = self.constants['slab_scale'] * jnp.sqrt(caux)\n",
                "        num = (c**2) * (lam**2)\n",
                "        denom = (c**2) + (tau**2) * (lam**2)\n",
                "        lambda_tilde = jnp.sqrt(num / denom)\n",
                "        beta = z * lambda_tilde * tau\n",
                "        return beta\n",
                "\n",
                "    def log_likelihood(self, data: Any, params: Dict[str, Any]) -> jnp.ndarray:\n",
                "        # USE DATA ARGUMENT, NOT SELF.X\n",
                "        X = jnp.asarray(data['X'], dtype=jnp.float64)\n",
                "        y = jnp.asarray(data['y'], dtype=jnp.float64)\n",
                "        \n",
                "        beta0, z, tau, lam, caux = self._get_params(params)\n",
                "        \n",
                "        target_ndim = z.ndim\n",
                "        def align_dims(x, target_rank):\n",
                "            while x.ndim < target_rank:\n",
                "                x = x[..., jnp.newaxis]\n",
                "            return x\n",
                "\n",
                "        tau = align_dims(tau, target_ndim)\n",
                "        caux = align_dims(caux, target_ndim)\n",
                "        lam = align_dims(lam, target_ndim)\n",
                "        \n",
                "        beta = self._compute_beta(z, tau, lam, caux)\n",
                "        \n",
                "        # Logits: X @ beta.T or einsum\n",
                "        # Check if we are broadcasting over LOO-expanded params (Rank 3) or standard (Rank 2)\n",
                "        if beta.ndim == 3: # (S, N_batch, D)\n",
                "             logits = jnp.einsum('nd,snd->sn', X, beta)\n",
                "             if beta0.ndim < 2: beta0 = align_dims(beta0, 2)\n",
                "             if beta0.ndim == 3: beta0 = beta0.squeeze(-1)\n",
                "             logits = logits + beta0\n",
                "        else: # (S, D)\n",
                "             logits = jnp.dot(beta, X.T) # (S, N_batch)\n",
                "             if beta0.ndim == 1: beta0 = beta0[:, jnp.newaxis]\n",
                "             logits = logits + beta0\n",
                "        \n",
                "        y_broad = y[jnp.newaxis, :]\n",
                "        ll = y_broad * logits - jnp.logaddexp(0.0, logits)\n",
                "        return ll\n",
                "\n",
                "    def unormalized_log_prob(self, data=None, **params):\n",
                "        # For prior, we don't depend on data, but likelihood part does.\n",
                "        # Usually AIS calls this with FULL data for 'log_ell_original'.\n",
                "        # If data is None, we default to full data stored in self.\n",
                "        if data is None:\n",
                "             data = {'X': self.X_full, 'y': self.y_full}\n",
                "             \n",
                "        ll_sum = jnp.sum(self.log_likelihood(data, params), axis=1)\n",
                "        \n",
                "        beta0, z, tau, lam, caux = self._get_params(params)\n",
                "        lp_z = jnp.sum(tfd.Normal(jnp.float64(0.0), jnp.float64(1.0)).log_prob(z), axis=1)\n",
                "        lp_beta0 = tfd.Normal(jnp.float64(0.0), self.constants['scale_icept']).log_prob(jnp.squeeze(beta0))\n",
                "        dist_tau = tfd.StudentT(df=self.constants['nu_global'], loc=jnp.float64(0.0), scale=self.constants['scale_global'])\n",
                "        lp_tau = dist_tau.log_prob(jnp.squeeze(tau)) + jnp.log(2.0) + params['log_tau'].squeeze()\n",
                "        dist_lam = tfd.StudentT(df=self.constants['nu_local'], loc=jnp.float64(0.0), scale=jnp.float64(1.0))\n",
                "        lp_lam = jnp.sum(dist_lam.log_prob(lam) + jnp.log(2.0), axis=1) + jnp.sum(params['log_lambda'], axis=1)\n",
                "        dist_caux = tfd.InverseGamma(concentration=0.5*self.constants['slab_df'], scale=0.5*self.constants['slab_df'])\n",
                "        lp_caux = dist_caux.log_prob(jnp.squeeze(caux)) + params['log_caux'].squeeze()\n",
                "        \n",
                "        return ll_sum + lp_z + lp_beta0 + lp_tau + lp_lam + lp_caux\n",
                "\n",
                "    def log_likelihood_gradient(self, data: Any, params: Dict[str, Any]) -> jnp.ndarray:\n",
                "        X = jnp.asarray(data['X'], dtype=jnp.float64)\n",
                "        y = jnp.asarray(data['y'], dtype=jnp.float64)\n",
                "        \n",
                "        def single_point_grad(theta_s, x_n, y_n):\n",
                "            p = self.reconstruct_parameters(theta_s, params)\n",
                "            beta0, z, tau, lam, caux = self._get_params(p)\n",
                "            beta = self._compute_beta(z, tau.squeeze(), lam, caux.squeeze())\n",
                "            logits = beta0.squeeze() + jnp.dot(beta, x_n)\n",
                "            ll = y_n * logits - jnp.logaddexp(0.0, logits)\n",
                "            return ll\n",
                "\n",
                "        grad_fn = jax.grad(single_point_grad)\n",
                "        theta = self.extract_parameters(params)\n",
                "        vmap_grad = jax.vmap(grad_fn, in_axes=(None, 0, 0), out_axes=0)\n",
                "        vvmap_grad = jax.vmap(vmap_grad, in_axes=(0, None, None), out_axes=0)\n",
                "        grads = vvmap_grad(theta, X, y)\n",
                "        return grads\n",
                "        \n",
                "    def log_likelihood_hessian_diag(self, data: Any, params: Dict[str, Any]) -> jnp.ndarray:\n",
                "        X = jnp.asarray(data['X'], dtype=jnp.float64)\n",
                "        y = jnp.asarray(data['y'], dtype=jnp.float64)\n",
                "        def single_point_hess(theta_s, x_n, y_n):\n",
                "            hess = jax.hessian(lambda t: \n",
                "                 self._single_point_ll_ad(t, x_n, y_n, params)\n",
                "            )(theta_s)\n",
                "            return jnp.diag(hess)\n",
                "        theta = self.extract_parameters(params)\n",
                "        vmap_hess = jax.vmap(single_point_hess, in_axes=(None, 0, 0), out_axes=0)\n",
                "        vvmap_hess = jax.vmap(vmap_hess, in_axes=(0, None, None), out_axes=0)\n",
                "        return vvmap_hess(theta, X, y)\n",
                "\n",
                "    def _single_point_ll_ad(self, theta_s, x_n, y_n, template):\n",
                "        p = self.reconstruct_parameters(theta_s, template)\n",
                "        beta0, z, tau, lam, caux = self._get_params(p)\n",
                "        beta = self._compute_beta(z, tau.squeeze(), lam, caux.squeeze())\n",
                "        logits = beta0.squeeze() + jnp.dot(beta, x_n)\n",
                "        ll = y_n * logits - jnp.logaddexp(0.0, logits)\n",
                "        return ll\n",
                "\n",
                "    def extract_parameters(self, params: Dict[str, Any]) -> jnp.ndarray:\n",
                "        beta0 = jnp.asarray(params['beta0'], dtype=jnp.float64)\n",
                "        log_tau = jnp.asarray(params['log_tau'], dtype=jnp.float64)\n",
                "        log_caux = jnp.asarray(params['log_caux'], dtype=jnp.float64)\n",
                "        z = jnp.asarray(params['z'], dtype=jnp.float64)\n",
                "        log_lambda = jnp.asarray(params['log_lambda'], dtype=jnp.float64)\n",
                "        if beta0.ndim == 1: beta0 = beta0[:, jnp.newaxis]\n",
                "        if log_tau.ndim == 1: log_tau = log_tau[:, jnp.newaxis]\n",
                "        if log_caux.ndim == 1: log_caux = log_caux[:, jnp.newaxis]\n",
                "        return jnp.concatenate([beta0, log_tau, log_caux, z, log_lambda], axis=1)\n",
                "\n",
                "    def reconstruct_parameters(self, flat_params: jnp.ndarray, template: Dict[str, Any]) -> Dict[str, Any]:\n",
                "        D = template['z'].shape[-1]\n",
                "        beta0 = flat_params[..., 0:1]\n",
                "        log_tau = flat_params[..., 1:2]\n",
                "        log_caux = flat_params[..., 2:3]\n",
                "        z = flat_params[..., 3:3+D]\n",
                "        log_lambda = flat_params[..., 3+D:]\n",
                "        return {'beta0': beta0, 'log_tau': log_tau, 'log_caux': log_caux, 'z': z, 'log_lambda': log_lambda}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Full Posterior...\n"
                    ]
                }
            ],
            "source": [
                "# -----------------------------------------------------\n",
                "# 2. Load Data and Posterior\n",
                "# -----------------------------------------------------\n",
                "with importlib.resources.path('bayesianquilts.data', \"overianx.csv\") as xpath:\n",
                "    X = pd.read_csv(xpath, header=None)\n",
                "with importlib.resources.path('bayesianquilts.data', \"overiany.csv\") as ypath:\n",
                "    y = pd.read_csv(ypath, header=None)\n",
                "X_scaled = (X - X.mean())/X.std()\n",
                "X_scaled = X_scaled.fillna(0)\n",
                "X_np = X_scaled.to_numpy(dtype=float)\n",
                "y_np = y.to_numpy(dtype=float).flatten()\n",
                "n = X_np.shape[0]\n",
                "p = X_np.shape[1]\n",
                "guessnumrelevcov = n / 10\n",
                "scale_global_val = guessnumrelevcov / ((p - guessnumrelevcov) * np.sqrt(n))\n",
                "constants = {'slab_scale': 2.5, 'scale_icept': 5.0, 'nu_global': 1.0, 'nu_local': 1.0, 'slab_df': 1.0, 'scale_global': scale_global_val}\n",
                "data_dict_full = {'X': X_np, 'y': y_np, 'constants': constants}\n",
                "\n",
                "print(\"Loading Full Posterior...\")\n",
                "fname_0 = os.path.join(data_dir, \"ovarian_loo_0.npy\")\n",
                "params_full = np.load(fname_0, allow_pickle=True).item()\n",
                "\n",
                "# AGGRESSIVE SUBSAMPLE for stability\n",
                "n_sub = 50\n",
                "idx = np.arange(n_sub)\n",
                "\n",
                "beta0 = jnp.asarray(np.squeeze(params_full['beta0'])[idx], dtype=jnp.float64)\n",
                "z = jnp.asarray(np.squeeze(params_full['z'])[idx], dtype=jnp.float64)\n",
                "tau = jnp.asarray(np.squeeze(params_full['tau'])[idx], dtype=jnp.float64)\n",
                "lam = jnp.asarray(np.squeeze(params_full['lambda'])[idx], dtype=jnp.float64)\n",
                "caux = jnp.asarray(np.squeeze(params_full['caux'])[idx], dtype=jnp.float64)\n",
                "\n",
                "# Log transforms\n",
                "log_tau = jnp.log(tau)\n",
                "log_lambda = jnp.log(lam)\n",
                "log_caux = jnp.log(caux)\n",
                "\n",
                "params_jax = {\n",
                "    'beta0': beta0, \n",
                "    'z': z, \n",
                "    'log_tau': log_tau, \n",
                "    'log_lambda': log_lambda, \n",
                "    'log_caux': log_caux\n",
                "}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running AIS with methods: ['identity', 'll', 'kl', 'var', 'mm1', 'mm2', 'pmm1', 'pmm2'], Batch Size: 2\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processing Batches:   0%|          | 0/27 [00:00<?, ?it/s]"
                    ]
                }
            ],
            "source": [
                "# -----------------------------------------------------\n",
                "# 3. Run Adaptive IS in Batches\n",
                "# -----------------------------------------------------\n",
                "\n",
                "model = OvarianReparametrizedLikelihood(data_dict_full, priors=None)\n",
                "ais = AdaptiveImportanceSampler(likelihood_fn=model)\n",
                "ais.model = model \n",
                "\n",
                "methods = ['identity', 'll', 'kl', 'var', 'mm1', 'mm2', 'pmm1', 'pmm2']\n",
                "rhos = [1/2**i for i in range(1, 7)]\n",
                "\n",
                "# Batching strategy\n",
                "batch_size = 2\n",
                "n_data = X_np.shape[0]\n",
                "batches = range(0, n_data, batch_size)\n",
                "\n",
                "accumulated_results = {m: [] for m in methods}\n",
                "\n",
                "print(f\"Running AIS with methods: {methods}, Batch Size: {batch_size}\")\n",
                "\n",
                "for start_idx in tqdm(batches, desc=\"Processing Batches\"):\n",
                "    end_idx = min(start_idx + batch_size, n_data)\n",
                "    \n",
                "    # Slice Data\n",
                "    X_batch = X_np[start_idx:end_idx]\n",
                "    y_batch = y_np[start_idx:end_idx]\n",
                "    data_batch = {'X': X_batch, 'y': y_batch, 'constants': constants}\n",
                "    \n",
                "    try:\n",
                "        batch_res = ais.adaptive_is_loo(\n",
                "            data=data_batch,\n",
                "            params=params_jax,\n",
                "            rhos=rhos,\n",
                "            variational=False,\n",
                "            transformations=methods\n",
                "        )\n",
                "        \n",
                "        for method in methods:\n",
                "            if method in batch_res:\n",
                "                accumulated_results[method].append(batch_res[method]['p_loo_psis'])\n",
                "    except Exception as e:\n",
                "        print(f\"Batch {start_idx}-{end_idx} failed: {e}\")\n",
                "\n",
                "# Concatenate Results\n",
                "final_results = {}\n",
                "for method in methods:\n",
                "    if accumulated_results[method]:\n",
                "        concatenated = np.concatenate([np.asarray(a) for a in accumulated_results[method]])\n",
                "        final_results[method] = {'p_loo_psis': concatenated}\n",
                "        \n",
                "print(\"AIS Batch Processing Complete.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Computing Ground Truth...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading GT Folds: 100%|██████████| 54/54 [00:03<00:00, 14.85it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Ground Truth Total LOO: -14.65 +/- 3.35\n"
                    ]
                },
                {
                    "ename": "NameError",
                    "evalue": "name 'final_results' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m summary = []\n\u001b[32m     38\u001b[39m summary.append({\u001b[33m'\u001b[39m\u001b[33mMethod\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mGround Truth\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTotal LOO\u001b[39m\u001b[33m'\u001b[39m: gt_total, \u001b[33m'\u001b[39m\u001b[33mSE\u001b[39m\u001b[33m'\u001b[39m: gt_se, \u001b[33m'\u001b[39m\u001b[33mDiff\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m})\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, res \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfinal_results\u001b[49m.items():\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key == \u001b[33m'\u001b[39m\u001b[33mbest\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# Note: final_results[key]['p_loo_psis'] should be shape (N_samples,)\u001b[39;00m\n",
                        "\u001b[31mNameError\u001b[39m: name 'final_results' is not defined"
                    ]
                }
            ],
            "source": [
                "# -----------------------------------------------------\n",
                "# 4. Compare with Ground Truth\n",
                "# -----------------------------------------------------\n",
                "\n",
                "n_samples = X_np.shape[0]\n",
                "loo_gt_elpd = np.zeros(n_samples)\n",
                "missing = []\n",
                "\n",
                "def log_bernoulli(y_true, f_logits):\n",
                "    return y_true * f_logits - jnp.logaddexp(0.0, f_logits)\n",
                "    \n",
                "print(\"Computing Ground Truth...\")\n",
                "\n",
                "for i in tqdm(range(1, n_samples + 1), desc=\"Loading GT Folds\"):\n",
                "    fname = os.path.join(data_dir, f\"ovarian_loo_{i}.npy\")\n",
                "    if os.path.exists(fname):\n",
                "        try:\n",
                "            p = np.load(fname, allow_pickle=True).item()\n",
                "            xi = X_np[i-1]\n",
                "            yi = y_np[i-1]\n",
                "            beta_sample = np.squeeze(p['beta'])\n",
                "            beta0_sample = np.squeeze(p['beta0'])\n",
                "            f = beta0_sample + np.dot(beta_sample, xi)\n",
                "            ll = log_bernoulli(yi, f)\n",
                "            loo_gt_elpd[i-1] = special.logsumexp(ll) - np.log(len(ll))\n",
                "        except Exception as e:\n",
                "             missing.append(i)\n",
                "    else: missing.append(i)\n",
                "\n",
                "gt_total = loo_gt_elpd.sum()\n",
                "# SE for Ground Truth (approximate using variance of single points against their mean)\n",
                "# SE = sqrt(N) * std(loo_gt_elpd, ddof=1)\n",
                "gt_se = np.sqrt(n_samples) * np.std(loo_gt_elpd, ddof=1)\n",
                "print(f\"Ground Truth Total LOO: {gt_total:.2f} +/- {gt_se:.2f}\")\n",
                "\n",
                "# Collect Results\n",
                "summary = []\n",
                "summary.append({'Method': 'Ground Truth', 'Total LOO': gt_total, 'SE': gt_se, 'Diff': 0})\n",
                "\n",
                "for key, res in final_results.items():\n",
                "    if key == 'best': continue\n",
                "    \n",
                "    # Note: final_results[key]['p_loo_psis'] should be shape (N_samples,)\n",
                "    p_loo = res['p_loo_psis']\n",
                "    \n",
                "    if len(p_loo) == n_samples:\n",
                "        # The sum is: sum(log(1/S sum(w))) for each i\n",
                "        # p_loo vector contains p(y_i|y_-i)\n",
                "        \n",
                "        elpd_vec = np.log(p_loo + 1e-100) # Safety epsilon\n",
                "        total = np.sum(elpd_vec)\n",
                "        se = np.sqrt(n_samples) * np.std(elpd_vec, ddof=1)\n",
                "        summary.append({\n",
                "            'Method': key, \n",
                "            'Total LOO': total, \n",
                "            'SE': se,\n",
                "            'Diff': total - gt_total\n",
                "        })\n",
                "    else:\n",
                "        print(f\"Method {key} has incomplete results: {len(p_loo)}/{n_samples}\")\n",
                "\n",
                "df = pd.DataFrame(summary)\n",
                "\n",
                "# Visualization: Publication Quality Forest Plot\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "plt.rcParams.update({'font.size': 12, 'font.family': 'sans-serif'})\n",
                "\n",
                "# Separation of groups\n",
                "baselines = ['Ground Truth', 'identity', 'll']\n",
                "proposed = ['kl', 'var', 'mm1', 'mm2', 'pmm1', 'pmm2']\n",
                "\n",
                "# Order map: Proposed Top, Baselines Bottom. \n",
                "# Since we plot bottom-up, higher order map index = higher in plot.\n",
                "# Let's put Baselines at the very bottom (Index 0..k)\n",
                "# Proposed above them.\n",
                "full_order = baselines + proposed\n",
                "order_map = {name: i for i, name in enumerate(full_order)}\n",
                "df['Order'] = df['Method'].map(order_map)\n",
                "df = df.sort_values('Order', ascending=True) # Sort for consistent plotting logic below\n",
                "\n",
                "plt.figure(figsize=(8, 4.5))\n",
                "\n",
                "# Plot Loop\n",
                "for i, row in df.iterrows():\n",
                "    method = row['Method']\n",
                "    y_val = row['Order'] # Use fixed y-positions based on group\n",
                "    x_val = row['Total LOO']\n",
                "    x_err = row['SE']\n",
                "    \n",
                "    # Style logic\n",
                "    if method in proposed:\n",
                "        color = '#D55E00' # Vermilion\n",
                "        fmt = 'o'\n",
                "    elif method == 'Ground Truth':\n",
                "        color = 'black'\n",
                "        fmt = 'D'\n",
                "    else: # Other baselines\n",
                "        color = '#0072B2' # Blue\n",
                "        fmt = 's'\n",
                "        \n",
                "    plt.errorbar(\n",
                "        x=x_val, \n",
                "        y=y_val, \n",
                "        xerr=x_err, \n",
                "        fmt=fmt, \n",
                "        color=color, \n",
                "        capsize=4, \n",
                "        markersize=6, \n",
                "        linewidth=1.5\n",
                "    )\n",
                "    \n",
                "plt.axvline(gt_total, color='black', linestyle=':', alpha=0.6)\n",
                "\n",
                "# Set Y-ticks based on what is in DF\n",
                "yticks = sorted(df['Order'].unique())\n",
                "yticklabels = [df[df['Order'] == y]['Method'].iloc[0] for y in yticks]\n",
                "plt.yticks(yticks, yticklabels)\n",
                "\n",
                "# Bold proposed methods\n",
                "ax = plt.gca()\n",
                "for tick in ax.get_yticklabels():\n",
                "    if tick.get_text() in proposed:\n",
                "        tick.set_fontweight('bold')\n",
                "        \n",
                "plt.xlabel('Total LOO-ELPD')\n",
                "plt.grid(axis='y', alpha=0.0) # Only x grid\n",
                "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('ais_reparam_comparison.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
